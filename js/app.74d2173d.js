(function(n){function e(e){for(var r,i,o=e[0],p=e[1],c=e[2],d=0,l=[];d<o.length;d++)i=o[d],Object.prototype.hasOwnProperty.call(a,i)&&a[i]&&l.push(a[i][0]),a[i]=0;for(r in p)Object.prototype.hasOwnProperty.call(p,r)&&(n[r]=p[r]);m&&m(e);while(l.length)l.shift()();return s.push.apply(s,c||[]),t()}function t(){for(var n,e=0;e<s.length;e++){for(var t=s[e],r=!0,o=1;o<t.length;o++){var p=t[o];0!==a[p]&&(r=!1)}r&&(s.splice(e--,1),n=i(i.s=t[0]))}return n}var r={},a={app:0},s=[];function i(e){if(r[e])return r[e].exports;var t=r[e]={i:e,l:!1,exports:{}};return n[e].call(t.exports,t,t.exports,i),t.l=!0,t.exports}i.m=n,i.c=r,i.d=function(n,e,t){i.o(n,e)||Object.defineProperty(n,e,{enumerable:!0,get:t})},i.r=function(n){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})},i.t=function(n,e){if(1&e&&(n=i(n)),8&e)return n;if(4&e&&"object"===typeof n&&n&&n.__esModule)return n;var t=Object.create(null);if(i.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:n}),2&e&&"string"!=typeof n)for(var r in n)i.d(t,r,function(e){return n[e]}.bind(null,r));return t},i.n=function(n){var e=n&&n.__esModule?function(){return n["default"]}:function(){return n};return i.d(e,"a",e),e},i.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},i.p="/rtmp-reports/";var o=window["webpackJsonp"]=window["webpackJsonp"]||[],p=o.push.bind(o);o.push=e,o=o.slice();for(var c=0;c<o.length;c++)e(o[c]);var m=p;s.push([0,"chunk-vendors"]),t()})({0:function(n,e,t){n.exports=t("56d7")},2368:function(n,e,t){},4763:function(n,e,t){"use strict";var r=t("c015"),a=t.n(r);a.a},"4fec":function(n,e,t){},"56d7":function(n,e,t){"use strict";t.r(e);t("cadf"),t("551c"),t("f751"),t("097d");var r=t("2b0e"),a=t("8c4f"),s=function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{attrs:{id:"app"}},[t("GridMenu")],1)},i=[],o=function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{attrs:{id:"grids"}},[t("Header"),t("Menu"),t("Main")],1)},p=[],c=function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{attrs:{id:"header"}},[t("header",[t("MenuBar"),t("h1",{attrs:{id:"app-title"}},[n._v("RTMP Implementation Reports")])],1)])},m=[],d=function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"menu-bar"},[t("a",{on:{click:n.slideSideMenu}},[t("i",{staticClass:"fas fa-bars"})])])},l=[],u={name:"MenuBar",methods:{slideSideMenu:function(){var n=document.getElementById("menu");if("hidden"==n.getAttribute("class"))n.setAttribute("class","shown");else{if("shown"!=n.getAttribute("class"))return;n.setAttribute("class","hidden")}}}},b=u,_=t("2877"),h=Object(_["a"])(b,d,l,!1,null,null,null),f=h.exports,g={name:"Header",components:{MenuBar:f}},S=g,k=(t("a901"),Object(_["a"])(S,c,m,!1,null,null,null)),T=k.exports,L=function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"hidden",attrs:{id:"menu"}},[t("MenuBar"),t("div",{attrs:{id:"menu-list"}},[t("nav",[t("ul",[n._m(0),n._m(1),t("li",[t("router-link",{attrs:{to:"/rtmp-reports/overview"}},[n._v("RTMP の概要")])],1)])])])],1)},v=[function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("li",[t("a",{attrs:{href:"https://t-matsudate.github.io"}},[n._v("Back to Portfolio")])])},function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("li",[t("a",{attrs:{href:"/rtmp-reports"}},[n._v("目次")])])}],F={name:"Menu",components:{MenuBar:f}},M=F,y=(t("4763"),Object(_["a"])(M,L,v,!1,null,null,null)),P=y.exports,R=function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{attrs:{id:"main"}},[t("main",[t("Article")],1)])},I=[],E=function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("router-view")},A=[],C={name:"Article"},w=C,N=Object(_["a"])(w,E,A,!1,null,null,null),O=N.exports,D={name:"Main",components:{Article:O}},B=D,j=(t("58c6"),Object(_["a"])(B,R,I,!1,null,null,null)),H=j.exports,U={name:"GridMenu",components:{Header:T,Menu:P,Main:H}},x=U,z=(t("6eee"),Object(_["a"])(x,o,p,!1,null,null,null)),G=z.exports,V={name:"App",components:{GridMenu:G}},W=V,K=Object(_["a"])(W,s,i,!1,null,null,null),Z=K.exports,q=function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("Report",[t("Author",{attrs:{date:n.published,author:n.author},scopedSlots:n._u([{key:"author",fn:function(){},proxy:!0}])}),t("ReportTitle",{attrs:{"report-title":n.title},scopedSlots:n._u([{key:"title",fn:function(){},proxy:!0}])}),t("Markdown",{attrs:{source:n.source}})],1)},$=[],X=function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("article",{attrs:{id:"report"}},[n._t("author"),n._t("title"),n._t("default")],2)},Y=[],J={name:"Report"},Q=J,nn=Object(_["a"])(Q,X,Y,!1,null,null,null),en=nn.exports,tn=function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("p",{attrs:{id:"author"}},[n._v("\n  "+n._s(n.author)),t("br"),n._v("\n  投稿日: "),t("time",{attrs:{datetime:n.date}},[n._v(n._s(n.date))])])},rn=[],an={name:"Author",props:{date:{type:String,required:!0,validator:function(n){return!isNaN(new Date(n))}},author:{type:String,required:!0}},mounted:function(){var n=document.getElementsByTagName("head")[0],e=document.createElement("meta"),t=document.createElement("meta"),r=["func","func-hs","func_hs","t-matsudate","t.matsudate","rtmp","RTMP","implementation","Implementation","実装"];e.setAttribute("property","og:article:published_time"),e.setAttribute("content",this.date),t.setAttribute("property","og:article:author"),t.setAttribute("content",this.author),n.appendChild(e),n.appendChild(t),r.forEach(function(e){var t=document.createElement("meta");t.setAttribute("property","og:article:tag"),t.setAttribute("content",e),n.appendChild(t)})}},sn=an,on=(t("eb39"),Object(_["a"])(sn,tn,rn,!1,null,null,null)),pn=on.exports,cn=function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("h1",{attrs:{id:"report-title"}},[n._v(n._s(n.reportTitle))])},mn=[],dn=(t("ac4d"),t("8a81"),t("ac6a"),{name:"ReportTitle",props:{reportTitle:{type:String,required:!0}},mounted:function(){var n=document.getElementsByTagName("head")[0],e=document.createElement("meta"),t=document.createElement("meta"),r=document.createElement("meta");document.title=this.reportTitle,e.setAttribute("property","og:title"),e.setAttribute("content",this.reportTitle),t.setAttribute("property","og:type"),t.setAttribute("content","article"),r.setAttribute("property","og:url"),r.setAttribute("content","https://t-matsudate.github.io/rtmp-reports/"),n.appendChild(e),n.appendChild(t),n.appendChild(r);var a=!0,s=!1,i=void 0;try{for(var o,p=document.querySelectorAll("#menu-list nav ul li a")[Symbol.iterator]();!(a=(o=p.next()).done);a=!0){var c=o.value;if(c.innerHTML===this.reportTitle){c.id="current-article";break}}}catch(m){s=!0,i=m}finally{try{a||null==p.return||p.return()}finally{if(s)throw i}}}}),ln=dn,un=(t("eaf1"),Object(_["a"])(ln,cn,mn,!1,null,null,null)),bn=un.exports,_n=function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"markdown-body",domProps:{innerHTML:n._s(n.marked)}})},hn=[],fn=t("d4cd"),gn=t.n(fn),Sn=t("e6f9"),kn=t.n(Sn),Tn=t("ff97"),Ln=t.n(Tn),vn=t("5121"),Fn=t.n(vn),Mn=t("1c78"),yn=t("7629"),Pn=t.n(yn),Rn=t("be03"),In=t.n(Rn),En=t("364f"),An=t.n(En),Cn=t("7ba6"),wn=t.n(Cn),Nn=t("54f6"),On=t.n(Nn),Dn=t("362d"),Bn=t.n(Dn),jn=t("15e0"),Hn=t.n(jn),Un=t("9fce"),xn=t.n(Un),zn=t("bc2e"),Gn=t.n(zn),Vn=t("8102"),Wn=t.n(Vn),Kn=t("1d7a"),Zn=t.n(Kn),qn=t("bc0f"),$n=t.n(qn),Xn=t("b725"),Yn=t.n(Xn),Jn=t("e8aa"),Qn=t.n(Jn),ne=t("1487"),ee=t.n(ne),te={name:"Markdown",props:{source:{type:String,required:!0}},data:function(){return{md:new gn.a({html:!0,linkify:!0,typographer:!0,highlight:function(n,e){if(e&&ee.a.getLanguage(e))try{return'<pre class="hljs"><code>'+ee.a.highlight(e,n).value+"</code></pre>"}catch(t){return t}return""}}).use(kn.a).use(Ln.a).use(Fn.a).use(Mn["a"],{permalink:!0,permalinkBefore:!0}).use(Pn.a,{includeLevel:[2,3,4,5]}).use(In.a).use(An.a).use(wn.a).use(On.a).use(Bn.a).use(Hn.a).use(xn.a,{enableRowspan:!0,enableMultilineRows:!0}).use(Gn.a,{root:"../markdowns"}).use(Wn.a).use(Zn.a).use($n.a).use(Yn.a).use(Qn.a)}},computed:{marked:function(){return this.md.render(this.source)}},mounted:function(){var n=document.getElementsByClassName("table-of-contents"),e=document.getElementById("menu"),t=document.createElement("div"),r=document.createElement("nav");t.id="submenu",r.innerHTML=n[0].innerHTML,t.appendChild(r),e.appendChild(t);for(var a=0;a<n.length;a++)n[a].innerHTML=null}},re=te,ae=(t("9b80"),Object(_["a"])(re,_n,hn,!1,null,null,null)),se=ae.exports,ie='[[toc]]\n\n## はじめに\n\n私は RTMP サーバを実装するにあたって, まず Adobe Systems Inc. が公式に発行しているドキュメントや既存の OSS 製品を参照した. しかし, それらには以下の問題があることがわかった.\n\n* 公式のドキュメントが 2012 年発行のものと古く, 既存製品の最新の通信手順/通信内容からかけ離れてきている.\n  * ブログ等の既出の実装記事/解説記事についても, 時と共に最新の実装や仕様からはかけ離れてしまうという問題がある.\n* 既存製品(OSS 製品等目視確認できる範囲に限る)についても実装箇所を整理しにくい部分がある.\n* また, 既存製品は用いたプログラミング言語やフレームワークによって実装内容に差異があり, プロトコルで共通化されている部分なのか製品側が独自に実装している部分なのかの区別をつけにくいことがある.\n\n上記を解消する方法の一つとして, 私は自分自身で実装を行いながら, 当該プロトコルのサーバサイド/クライアントサイド両面の概要, 処理手順, 実装内容, およびそれらを解説する情報を随時更新していこうと考えた.\n\n## RTMP とは\n\nRTMP とは, TCP 上で映像や音声の送受信を行うプロトコルの 1 つである. Adobe Systems Inc. によって, 当時の Flash Player および Adobe Media Server 間で帯域の圧迫を避けながら映像/音声パケットを効率よく送受信するためのパケットのフォーマットおよび当該パケットの送受信手順について取り決められている.  \nまた,  RTMP には同じ手段で通信するものとして以下のような派生プロトコルも存在している.\n\n* RTMPE: 送信側が RTMP のハンドシェイクパケットを DH 暗号により暗号化してから通信を行うプロトコルである. ただし, あくまでパケットの暗号化であり通信経路自体は保護されていないため, 中間者攻撃等によりパケットごとすり替えられる脆弱性が存在する.\n* RTMPS: RTMP に TLS/SSL による暗号化および接続手順を合成したプロトコルである. TLS/SSL の証明書を付随させることにより、中間者攻撃等の被害を受けるリスクは軽減されている.\n* RTMPT/RTMPTE/RTMPTS: RTMP/RTMPE の通信を HTTP/HTTPS 上で行うプロトコルである. これらは RTMP に依存しない各種マネージドサービスとの連携による負荷分散が可能であり, 特に RTMPTS は HTTPS によって通信経路が保護されているため, RTMP/RTMPE はもとより RTMPS と比べてもセキュリティの信頼性が高いプロトコルと言える. \n\nRTMP および上記派生プロトコルのいずれも本質的には\n\n1. 当該プロトコルとしてのハンドシェイクを成立させる.\n2. アプリケーション間接続に必要な情報を相互に伝達しあう.\n3. 単位あたりのデータ量を帯域を圧迫しない程度に抑えつつ, 映像/音声データの送受信を行う.\n\nの 3 つの要件が大前提である. また, 上記 3 要件を満たせればよいことから, 近年では SIP 等の通話向けプロトコルの代替としての採用も確認され始めている.\n\n### 当該プロトコルを採用している製品(OSS)\n\n* [Red5](https://github.com/Red5/red5-server/)\n\nJava 言語で実装されたマルチメディアサーバである. RTMP を始め HLS による MPEG2-TS の転送や WebSocket を利用したコミュニケーションにも対応している. 有償ではあるがより高度な機能を搭載した Pro 版も存在している.\n\n* [FFmpeg](https://github.com/FFmpeg/FFmpeg/)\n\n動画や音声のエンコーダソフトである. C 言語で実装されている. 本来は動画や音声の変換がメインであるが, RTMP の登場に合わせて当該プロトコルでの外部サーバへの映像データの送信に対応した.  \nまた, ffserver というパッケージを導入することによってサーバとしての動きにも対応可能である. \n\n* [OBS](https://github.com/obsproject/obs-studio/)\n\nクライアントとしての映像/音声の配置や送信に注力したソフトウェアである. C 言語で実装されている. Windows を始め MacOS や Linux 等各種 OS に対してそれぞれパッケージが用意されているため, マルチプラットフォームと言っても差し支えないと思われる.\n\n## RTMP 接続の手順\n\n<div id="rtmp-connection-flows"></div>\n\n図1. RTMPの大まかな流れ {#caption-rtmp-connection-flows}\n\n1. サーバ側は TCP の 1935 番ポートを開放し, クライアント側からの接続を待ち受ける.\n2. クライアント側はサーバ側に TCP での接続を受理されたなら, TCP ハンドシェイクの後に RTMP 層でのハンドシェイクを行う.  \n(TCP パケットの受信方法や TCP ハンドシェイクの実装がまだである場合は, それも行う必要がある.)\n3. クライアント側はサーバ側に送信したハンドシェイクチャンクが妥当であると判断されたなら, RTMP 層でのアプリケーション接続を開始する.\n4. アプリケーション接続に成功したなら, サーバ側はクライアント側とやり取りするメッセージストリームに一意に ID を割り当てる.\n5. 映像/音声チャンクの送受信を開始する.\n\n### RTMP ハンドシェイク\n\nRTMP ハンドシェイクの手順は公式ドキュメント[^RTMP-Specification-1.0]では以下のように定義されている.\n\n<div id="rtmp-handshake-sequences-official">\n\n@startuml\n== 未初期化 ==\nClient -> Network: C0\nNetwork -> Server: C0\nClient -> Network: C1\nnote left\n    RTMP バージョンが\n    送信された.\nend note\nServer -> Network: S0\nServer -> Network: S1\nnote right\n    RTMP バージョンが\n    送信された.\nend note\nNetwork -> Client: S0\nNetwork -> Client: S1\nNetwork -> Server: C1\nClient -> Network: C2\nServer -> Network: S2\n== 肯定応答が送信された. ==\nNetwork -> Client: S2\nNetwork -> Server: C2\n== ハンドシェイクが完了した. ==\n@enduml\n\n</div>\n\n図2. 公式ドキュメントが説明している RTMP ハンドシェイクのシーケンス {#caption-rtmp-handshake-sequences-official}\n\n> 5.2.1.  Handshake Sequence\n>\n> The handshake begins with the client sending the C0 and C1 chunks.\n>\n> The client MUST wait until S1 has been received before sending C2.\n> The client MUST wait until S2 has been received before sending any other data.\n>\n> The server MUST wait until C0 has been received before sending S0 and S1, and MAY wait until after C1 as well.\n> The server MUST wait until C1 has been received before sending S2.\n> The server MUST wait until C2 has been received before sending any other data.\n\n\n* ハンドシェイクはクライアント側がサーバ側に C0 チャンクと C1 チャンクを送信することで始まる.\n* クライアント側は C2 チャンクの送信前に S1 の受信を待た**なければならない**.\n* クライアント側はその後の他のチャンクの送信前に S2 チャンクの受信を待た**なければならない**.\n* サーバ側は S2 チャンクの送信前に C1 チャンクの受信を待た**なければならない**.\n* サーバ側はその後の他のチャンクの送信前に C2 チャンクの受信を待た**なければならない**.\n\n> The following describes the states mentioned in the handshake diagram:\n>\n> Uninitialized: The protocol version is sent during this stage. Both the client and server are uninitialized. The The client sends the protocol version in packet C0. If the server supports the version, it sends S0 and S1 in response. If not, the server responds by taking the appropriate action. In RTMP, this action is terminating the connection.  \n> Version Sent:  Both client and server are in the Version Sent state after the Uninitialized state. The client is waiting for the packet S1 and the server is waiting for the packet C1. On receiving the awaited packets, the client sends the packet C2 and the server sends the packet S2. The state then becomes Ack Sent.  \n> Ack Sent: The client and the server wait for S2 and C2 respectively.  \n> Handshake Done: The client and the server exchange messages.\n\n* 未初期化\n\nプロトコルのバージョンが送信される. クライアント側もサーバ側も未初期化である. クライアント側はプロトコルのバージョンを C0 パケットで送信する. サーバ側はそのバージョンをサポートしているならば, クライアント側に応答メッセージで S0 パケットと S1 パケットを送信する. そうでなければ, サーバ側は適切なアクションをとって応答メッセージを送信する. RTMP では, そのアクションは接続の終了である.\n\n* RTMP バージョンが送信された\n\nサーバ側もクライアント側も未初期化状態の後は RTMP バージョンが送信された状態である. クライアント側は S1 パケットを待ちサーバ側は C1 パケットを待つ. 待機パケットの受信時に, クライアント側はサーバ側に C2 パケットを送信し, サーバ側はクライアント側に S2 パケットを送信する. それから肯定応答が送信された状態になる.\n\n* 肯定応答が送信された\n\nクライアント側とサーバ側はそれぞれ S2 と C2 を待つ.\n\n* ハンドシェイクが完了した\n\nクライアント側とサーバ側はメッセージを交換する.\n\n各種チャンクのフィールドは, 公式ドキュメント[^RTMP-Specification-1.0]では以下のように定義されている.\n\n#### C0 チャンクおよび S0 チャンク\n\n1. 利用する RTMP のバージョン(1 byte)\n\n> In C0, this field identifies the RTMP version requested by the client.\n> In S0, this field identifies the RTMP version selected by the server.\n> The version defined by this specification is 3.\n> Values 0-2 are deprecated values used by earlier proprietary products; 4-31 are reserved for future implementations; and 32-255 are not allowed (to allow distinguishing RTMP from text-based protocols, which always start with a printable character).\n> A server that does not recognize the client’s requested version SHOULD respond with 3.\n> The client MAY choose to degrade to version 3, or to abandon the handshake.\n\n* 双方が利用する RTMP のバージョンを指定する.\n* 基本的に, 指定できるバージョンは 3 である.\n  * 0 から 2 は本リリース前の企業製品によって使用されていたため非推奨である.\n  * 4 から 31 は未来のために予約している.\n  * 31 より大きい数はそもそも認めていない.\n* サーバ側は, クライアント側から要求されたバージョンを認識できない時は 3 と**すべきである**.\n* その場合, クライアント側はバージョンを 3 にグレードダウンするか接続を中止するかを選んで**よい**.\n\nとされているが, 2019 年現在このフィールドに指定できるバージョンは以下の通りである.\n\n* 3(RTMP)\n* 6(RTMPE)\n\n以下は Red5 および OBS が認識しているバージョンである.\n\n* 8(RTMPE/XTEA)\n* 9(RTMPE/Blowfish)\n\n以下に各 OSS 製品の該当部分の実装を示す.\n\nFFmpeg/rtmpproto.c#L1200-L1236[^FFmpeg/rtmpproto.c#L1200-L1236]\n\n```c\nuint8_t tosend [RTMP_HANDSHAKE_PACKET_SIZE+1] = {\n    3,                // unencrypted data\n    0, 0, 0, 0,       // client uptime\n    RTMP_CLIENT_VER1,\n    RTMP_CLIENT_VER2,\n    RTMP_CLIENT_VER3,\n    RTMP_CLIENT_VER4,\n};\n\n// 中略\n\nif (CONFIG_FFRTMPCRYPT_PROTOCOL && rt->encrypted) {\n    /* When the client wants to use RTMPE, we have to change the command\n     * byte to 0x06 which means to use encrypted data and we have to set\n     * the flash version to at least 9.0.115.0. */\n    tosend[0] = 6;\n    tosend[5] = 128;\n    tosend[6] = 0;\n    tosend[7] = 3;\n    tosend[8] = 2;\n\n    /* Initialize the Diffie-Hellmann context and generate the public key\n     * to send to the server. */\n    if ((ret = ff_rtmpe_gen_pub_key(rt->stream, tosend + 1)) < 0)\n        return ret;\n}\n```\n\nobs-studio/rtmp.c#L4062[^obs-studio/rtmp.c#L4062]\n\n```c\nclientbuf[0] = 0x03;\t\t/* not encrypted */\n```\n\nobs-studio/handshake.h#L831-L837[^obs-studio/handshake.h#L831-L837]\n\n```c\nif (encrypted)\n{\n    clientsig[-1] = 0x06;\t/* 0x08 is RTMPE as well */\n    offalg = 1;\n}\nelse\n    clientsig[-1] = 0x03;\n```\n\nred5-server-common/RTMPHandshake.java#L67[^red5-server-common/RTMPHandshake.java#L67]\n\n```java\npublic final static String[] HANDSHAKE_TYPES = {"Undefined0", "Undefined1", "Undefined2", "RTMP", "Undefined4", "Undefined5", "RTMPE", "Undefined7", "RTMPE XTEA", "RTMPE BLOWFISH"};\n```\n\n#### C1 チャンクおよび S1 チャンク\n\n1. タイムスタンプ(4 bytes)\n\n> This field contains a timestamp, which SHOULD be used as the epoch for all future chunks sent from this endpoint.\n> This may be 0, or some arbitrary value.\n> To synchronize multiple chunkstreams, the endpoint may wish to send the current value of the other chunkstream’s timestamp.\n\n* 今後送られることになるすべてのチャンクのタイムスタンプの基準として使われる**べきである**.\n* これは 0 でもよいし, 何らかの任意の値でもよい.\n* 複数のチャンクの同期のために, 現在のタイムスタンプを送信したりすることにも使える.\n\n2. ゼロ埋め(4 bytes)\n\n> This field MUST be all 0s.\n\n* すべて 0 で**なければならない**.\n\nとされているが, 2019 年現在ここには利用している Flash Player/Adobe Media Server のバージョンが割り当てられている. 以下に各 OSS 製品の該当部分の実装を示す.\n\nC1 チャンクの場合:\n\nFFmpeg/rtmpproto.c#L1200-L1207[^FFmpeg/rtmpproto.c#L1200-L1207]\n\n```c\nuint8_t tosend    [RTMP_HANDSHAKE_PACKET_SIZE+1] = {\n    3,                // unencrypted data\n    0, 0, 0, 0,       // client uptime\n    RTMP_CLIENT_VER1,\n    RTMP_CLIENT_VER2,\n    RTMP_CLIENT_VER3,\n    RTMP_CLIENT_VER4,\n};\n```\n\nFFmpeg/rtmp.h#L32-L41[^FFmpeg/rtmp.h#L32-L41]\n\n```c\n/**\n * emulated Flash client version - 9.0.124.2 on Linux\n * @{\n */\n#define RTMP_CLIENT_PLATFORM "LNX"\n#define RTMP_CLIENT_VER1    9\n#define RTMP_CLIENT_VER2    0\n#define RTMP_CLIENT_VER3  124\n#define RTMP_CLIENT_VER4    2\n/** @} */ //version defines\n```\n\nobs-studio/handshake.h#L842-L865[^obs-studio/handshake.h#L842-L865]\n\n```c\nif (FP9HandShake)\n{\n    /* set version to at least 9.0.115.0 */\n    if (encrypted)\n    {\n        clientsig[4] = 128;\n        clientsig[6] = 3;\n    }\n    else\n    {\n        clientsig[4] = 10;\n        clientsig[6] = 45;\n    }\n    clientsig[5] = 0;\n    clientsig[7] = 2;\n\n    RTMP_Log(RTMP_LOGDEBUG, "%s: Client type: %02X", __FUNCTION__, clientsig[-1]);\n    getdig = digoff[offalg];\n    getdh  = dhoff[offalg];\n}\nelse\n{\n    memset(&clientsig[4], 0, 4);\n}\n```\n\nS1 チャンクの場合:\n\nred5-server/InboundHandshake.java#L348-L352[^red5-server/InboundHandshake.java#L348-L352]\n\n```java\n// version 4\nhandshakeBytes[4] = 4;\nhandshakeBytes[5] = 0;\nhandshakeBytes[6] = 0;\nhandshakeBytes[7] = 1;\n```\n\nこれは, Flash Player 9 および Adobe Media Server 3 前後でハンドシェイクの手順や実装内容に変更が加えられているため, どのバージョンのハンドシェイクを利用するかを区別するために存在する.\n\n3. ランダムなバイト列(1528 bytes)\n\n> This field can contain any arbitrary values.\n> Since each endpoint has to distinguish between the response to the handshake it has initiated and the handshake initiated by its peer, this data SHOULD send something sufficiently random.\n> But there is no need for cryptographically-secure randomness, or even dynamic values.\n\n* このフィールドはあらゆる任意の値を含むことができる.\n* 送受信する相手を区別しなければいけないため, このフィールドの値は十分にランダムである**べき**だが, それが暗号的に安全であったり動的な値である必要はない. \n\n<a name="fp9"></a>\nとされているが, 2019 年現在これは単にランダムな値ではなく, Flash Player 9 および Adobe Media Server 3 以降は送信時に HMAC-SHA256 のダイジェストを埋め込むようになっている. ダイジェストの位置は C0 チャンクおよび S0 チャンクで指定された RTMP のバージョンによって差異がある.  \nそれらの位置はそれぞれ以下の計算式で求めることができる.\n\nランダムなバイト列を$R$とおく.\n\nRTMP(3) の場合:\n\n$\\displaystyle\\sum_{i=0}^4 R_{i}\\mod 728 + 12$\n\nRTMPE(6, 8 および 9) の場合:\n\n$\\displaystyle\\sum_{i=764}^4 R_{i}\\mod 728 + 776$\n\nここで, ダイジェスト生成に使う鍵はクライアント/サーバ側でそれぞれ以下の通りである.\n\nクライアント側:\n\n* C1 チャンクの送信時\n\n"Genuine Adobe Flash Player 001"\n\n* 返送された C1 チャンクの受信時\n\n"Genuine Adobe Flash Player 001 **0x**F0EEC24A8068BEE82E00D0D1029E7E576EEC5D2D29806FAB93B8E636CFEB31AE"\n\nサーバ側:\n\n* S1 チャンクの送信時\n\n"Genuine Adobe Flash Media Server 001"\n\n* 返送された S1 チャンクの受信時\n\n"Genuine Adobe Flash Media Server 001 **0x**F0EEC24A8068BEE82E00D0D1029E7E576EEC5D2D29806FAB93B8E636CFEB31AE"\n\n#### C2 チャンクおよび S2 チャンク\n\n1. タイムスタンプ(4 bytes)\n\n> This field MUST contain the timestamp sent by the peer in S1 (for C2) or C1 (for S2).\n\n* このフィールドはお互いに相手の第一チャンクが**送られた**時点のタイムスタンプを含め**なければならない**.\n\n2. タイムスタンプ(4 bytes)\n\n> This field MUST contain the timestamp at which the previous packet(s1 or c1) sent by the peer was read.\n\n* このフィールドはお互いに相手から送られた第一チャンクを**読み込んだ**時点のタイムスタンプを含め**なければならない**.\n\n3. ランダムなバイト列の**エコー**(1528 bytes)\n\n> This field MUST contain the random data field sent by the peer in S1 (for C2) or S2 (for C1).\n> Either peer can use the time and time2 fields together with the current timestamp as a quick estimate of the bandwidth and/or latency of the connection, but this is unlikely to be useful.\n\n* このフィールドは C2 チャンクの場合は S1 チャンクによって送られたランダムなバイト列を, C1 チャンクの場合は S2 チャンクによって送られたランダムなバイト列を含め**なければならない**.\n* どちら側も 2 つのタイムスタンプを接続の帯域幅や待ち時間の簡易な見積もりとして使えるが, あまり役に立たない.\n\n公式ドキュメントの文言だけではわかりにくいが, 各種 OSS 製品の実装の中にその答えがあったので以下に示す.\n\nクライアント側の場合:\n\nFFmpeg/rtmpproto.c#L1248-L1258[^FFmpeg/rtmpproto.c#L1248-L1258]\n\n```c\nif ((ret = ffurl_read_complete(rt->stream, serverdata,\n                               RTMP_HANDSHAKE_PACKET_SIZE + 1)) < 0) {\n    av_log(s, AV_LOG_ERROR, "Cannot read RTMP handshake response\\n");\n    return ret;\n}\n\nif ((ret = ffurl_read_complete(rt->stream, clientdata,\n                               RTMP_HANDSHAKE_PACKET_SIZE)) < 0) {\n    av_log(s, AV_LOG_ERROR, "Cannot read RTMP handshake response\\n");\n    return ret;\n}\n```\n\nobs-studio/rtmp.c#L4089-L4112[^obs-studio/rtmp.c#L4089-L4112]\n\n```c\nif (ReadN(r, serversig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE)\n    return FALSE;\n\n/* decode server response */\n\nmemcpy(&suptime, serversig, 4);\nsuptime = ntohl(suptime);\n\nRTMP_Log(RTMP_LOGDEBUG, "%s: Server Uptime : %d", __FUNCTION__, suptime);\nRTMP_Log(RTMP_LOGDEBUG, "%s: FMS Version   : %d.%d.%d.%d", __FUNCTION__,\n         serversig[4], serversig[5], serversig[6], serversig[7]);\n\n/* 2nd part of handshake */\nif (!WriteN(r, serversig, RTMP_SIG_SIZE))\n    return FALSE;\n\nif (ReadN(r, serversig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE)\n    return FALSE;\n\nbMatch = (memcmp(serversig, clientsig, RTMP_SIG_SIZE) == 0);\nif (!bMatch)\n{\n    RTMP_Log(RTMP_LOGWARNING, "%s, client signature does not match!", __FUNCTION__);\n}\n```\n\nobs-studio/handshake.h#L936-L945[^obs-studio/handshake.h#L936-L945]\nobs-studio/handshake.h#L1078-L1083[^obs-studio/handshake.h#L1078-L1083]\nobs-studio/handshake.h#L1170-L1174[^obs-studio/handshake.h#L1170-L1174]\n\n```c\nif (ReadN(r, (char *)serversig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE)\n    return FALSE;\n\n/* decode server response */\nmemcpy(&uptime, serversig, 4);\nuptime = ntohl(uptime);\n\nRTMP_Log(RTMP_LOGDEBUG, "%s: Server Uptime : %d", __FUNCTION__, uptime);\nRTMP_Log(RTMP_LOGDEBUG, "%s: FMS Version   : %d.%d.%d.%d", __FUNCTION__, serversig[4],\n         serversig[5], serversig[6], serversig[7]);\n\n// 中略\n\nif (!WriteN(r, (char *)reply, RTMP_SIG_SIZE))\n    return FALSE;\n\n/* 2nd part of handshake */\nif (ReadN(r, (char *)serversig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE)\n    return FALSE;\n\n// 中略\n\nif (memcmp(serversig, clientsig, RTMP_SIG_SIZE) != 0)\n{\n    RTMP_Log(RTMP_LOGWARNING, "%s: client signature does not match!",\n             __FUNCTION__);\n}\n```\n\nサーバ側の場合:\n\nFFmpeg/rtmpproto.c#L1452-L1472[^FFmpeg/rtmpproto.c#L1452-L1472]\n\n```c\n/* Send S1 */\n/* By now same epoch will be sent */\nhs_my_epoch = hs_epoch;\n/* Generate random */\nfor (randomidx = 8; randomidx < (RTMP_HANDSHAKE_PACKET_SIZE);\n     randomidx += 4)\n    AV_WB32(hs_s1 + randomidx, av_get_random_seed());\n\nret = rtmp_send_hs_packet(rt, hs_my_epoch, 0, hs_s1,\n                          RTMP_HANDSHAKE_PACKET_SIZE);\nif (ret) {\n    av_log(s, AV_LOG_ERROR, "RTMP Handshake S1 Error\\n");\n    return ret;\n}\n/* Send S2 */\nret = rtmp_send_hs_packet(rt, hs_epoch, 0, hs_c1,\n                          RTMP_HANDSHAKE_PACKET_SIZE);\nif (ret) {\n    av_log(s, AV_LOG_ERROR, "RTMP Handshake S2 Error\\n");\n    return ret;\n}\n```\n\nobs-studio/rtmp.c#L4152-L4178[^obs-studio/rtmp.c#L4152-L4178]\n\n```c\nif (!WriteN(r, serverbuf, RTMP_SIG_SIZE + 1))\n    return FALSE;\n\nif (ReadN(r, clientsig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE)\n    return FALSE;\n\n/* decode client response */\n\nmemcpy(&uptime, clientsig, 4);\nuptime = ntohl(uptime);\n\nRTMP_Log(RTMP_LOGDEBUG, "%s: Client Uptime : %d", __FUNCTION__, uptime);\nRTMP_Log(RTMP_LOGDEBUG, "%s: Player Version: %d.%d.%d.%d", __FUNCTION__,\n         clientsig[4], clientsig[5], clientsig[6], clientsig[7]);\n\n/* 2nd part of handshake */\nif (!WriteN(r, clientsig, RTMP_SIG_SIZE))\n    return FALSE;\n\nif (ReadN(r, clientsig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE)\n    return FALSE;\n\nbMatch = (memcmp(serversig, clientsig, RTMP_SIG_SIZE) == 0);\nif (!bMatch)\n{\n    RTMP_Log(RTMP_LOGWARNING, "%s, client signature does not match!", __FUNCTION__);\n}\n```\n\nobs-studio/hansdhake.h#L1442-L1447[^obs-studio/handshake.h#L1442-L1447]\nobs-studio/handshake.h#L1524-L1528[^obs-studio/handshake.h#L1524-L1528]\n\n```c\nif (!WriteN(r, (char *)clientsig, RTMP_SIG_SIZE))\n    return FALSE;\n\n/* 2nd part of handshake */\nif (ReadN(r, (char *)clientsig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE)\n    return FALSE;\n\n// 中略\n\nif (memcmp(serversig, clientsig, RTMP_SIG_SIZE) != 0)\n{\n    RTMP_Log(RTMP_LOGWARNING, "%s: client signature does not match!",\n             __FUNCTION__);\n}\n```\n\nred5-server/InboundHandshake.java#L213-L224[^red5-server/InboundHandshake.java#L213-L224]\nred5-server/InboundHandshake.java#L304-L306[^red5-server/InboundHandshake.java#L304-L306]\n\n```java\nIoBuffer s0s1s2 = IoBuffer.allocate(Constants.HANDSHAKE_SIZE * 2 + 1); // 3073\n// set handshake with encryption type\ns0s1s2.put(handshakeType); // 1\ns0s1s2.put(s1); // 1536\ns0s1s2.put(c1); // 1536\ns0s1s2.flip();\n// clear original base bytes\nhandshakeBytes = null;\nif (log.isTraceEnabled()) {\n    log.trace("S0+S1+S2 size: {}", s0s1s2.limit());\n}\nreturn s0s1s2;\n\n// 中略\n\nif (!Arrays.equals(s1, c2)) {\n    log.info("Client signature doesn\'t match!");\n}\n```\n\n用いているプログラミング言語の違い等によって実装内容に差異はあるものの, 上記の各種実装を参考にすると以下に要約できる.\n\n* C2 チャンク: S1 チャンクと同じ内容を書き込み, 送信する.\n* S2 チャンク: C1 チャンクと同じ内容を書き込み, 送信する.\n\nただし, Flash Player 9 および Adobe Media Server 3 以上の場合は C1 チャンクおよび S1 チャンクのランダムバイト列の所定の位置を HMAC-SHA256 で求めたダイジェストに置き換えて送受信を行い, 受信時に[ダイジェストの位置を探し当てて](#fp9)送信前のダイジェストと照合することでメッセージの正当性を検証する必要がある.\n\n上記の各実装より, 現在の RTMP 層におけるハンドシェイクの手順は以下に要約できる.\n\n<div id="rtmp-handshake-sequences-current">\n\n@startuml\nクライアント -> サーバ: C0+C1\nサーバ -> クライアント: S0+S1+S2\nクライアント -> サーバ: C2\n== アプリケーション接続へ ==\n@enduml\n\n</div>\n\n図3. 現在の RTMP ハンドシェイクの手順 {#caption-rtmp-handshake-sequences-current}\n\n1. クライアント側はサーバ側に C0 チャンクと C1 チャンクをそれぞれ送信する.\n2. サーバ側はクライアント側から C0 チャンク と C1 チャンクをそれぞれ受信したなら, S0 チャンク, S1 チャンクおよび S2 チャンクをそれぞれクライアント側に送信する.\n3. クライアント側はサーバ側から S0 チャンク, S1 チャンクおよび S2 チャンクをそれぞれ受信したなら, C2 チャンクをサーバ側へ送信する.\n4. サーバ側はクライアント側から C2 チャンクを受け取ったなら, アプリケーション接続に移行する.\n\n### Invoke(connect) から映像データの受信まで\n\nRTMP 層におけるハンドシェイクが完了したなら, サーバ側とクライアント側は映像の送受信に必要な情報を相互に伝達しあう. それは以下の手順で行う.\n\n<div id="rtmp-application-connect-sequences">\n\n@startuml\n== ハンドシェイクが完了した. ==\nクライアント -> サーバ: Invoke(connect)\nサーバ -> クライアント: Invoke(_result)\nクライアント -> サーバ: Invoke(createStream)\nサーバ -> クライアント: Invoke(_result)\nクライアント -> サーバ: Invoke(publish)\nサーバ -> クライアント: Invoke(onStatus)\n== 映像・音声データの送受信へ ==\n@enduml\n\n</div>\n\n図4. アプリケーション接続の大まかなシーケンス {#caption-rtmp-application-connect-sequences}\n\n1. クライアント側はサーバ側に Invoke(connect) メッセージを送信する.\n2. サーバ側はクライアント側から受信した Invoke(connect) メッセージをデコードし, 応答メッセージをクライアント側に送信する.\n3. クライアント側はサーバ側から Invoke(\\_result) を受信したなら, Invoke(createStream) メッセージをサーバ側に送信し, メッセージストリームへの一意な ID の付番を要求する.\n4. サーバ側はクライアント側から受信した Invoke(createStream) メッセージをデコードし, 応答メッセージをクライアント側に送信する.\n5. クライアント側はサーバ側から Invoke(\\_result) を受信したなら, Invoke(publish) をサーバ側に送信し, 映像の送信開始を伝える.\n6. サーバ側はクライアント側から受信した Invoke(publish) をデコードし, 応答メッセージをクライアント側に送信する.\n7. クライアント側はサーバ側から Invoke(onStatus) を受信したなら, 映像/音声の送信を開始する.\n\nなお, RTMP ハンドシェイク以降に送受信されるチャンクの構造は以下の通りである. ここから Big Endian と Little Endian の違いを考慮していく必要があるので注意が必要である.\n\n#### メッセージチャンクの構造\n\nハンドシェイク後に送受信されるチャンクは公式ドキュメント[^RTMP-Specification-1.0]では以下のように定義されている.\n\n1. チャンクベーシックヘッダ (最大 3 bytes)\n\n* チャンクメッセージヘッダのフォーマット (2 **bits**)\n  * 後に続くチャンクメッセージヘッダのパターンを入力する.\n  * 0b00: Type 0 (11 bytes)\n  * 0b01: Type 1 (7 bytes)\n  * 0b10: Type 2 (3 bytes)\n  * 0b11: Type 3 (0 byte)\n* チャンクストリーム ID (6 **bits**, 1 byte および 2 bytes)\n  * チャンクメッセージの**種類**に応じて割り当てられる. 一意になるとは限らない.\n  * ID が 3 以上 63 以下である場合は 6 bits に収めて 1 byte にまとめる.\n  * 64 以上 319 以下である場合は 1 byte で表現する. その場合はフォーマット直後の 6 bits には 0 を入力する.\n  * 320 以上 65599 以下である場合は 2 bytes で表現する. その場合はフォーマット直後の 6 bits には 1 を入力する.\n  * 64 以上の場合は実際の ID よりも 64 少ないものとして扱われるため, デコード後に 64 を加えてから利用する必要がある.\n  * 320 以上の場合に限り当該 ID のバイト順序が **Little Endian** であるため, このパターンをデコードする際は更に注意しなければならない.\n  * なお, 3 未満の ID は予約済みである.\n\n2. チャンクメッセージヘッダ (最大 11 bytes. 0 byte を含む.)\n\nチャンクメッセージヘッダのパターンは, 上述のチャンクベーシックヘッダの上位 2 bits の値に応じて以下の 4 パターンに分けられる.\n\nType 0 (11 bytes):\n\nチャンクストリームの始まりはこの Type 0 パターンで**なければならない**.\n\n* タイムスタンプ (3 bytes)\n  * チャンクストリームの送信を開始した時点のタイムスタンプを入力する.\n  * タイムスタンプが 0xFFFFFF より大きくなる場合は後述の拡張タイムスタンプフィールドに入力し, このフィールドの値を 0xFFFFFF で固定する.\n* メッセージ長 (3 bytes)\n  * チャンクデータの長さを入力する. ただし, チャンクデータ自体の長さしか考慮されていない. (詳細は後述する)\n* メッセージ種類 ID (1 byte)\n  * 後に続くチャンクデータの種類を入力する. 現在仕様書に存在している, または利用が確認されている種類は[メッセージの種類](#メッセージの種類)を参照.\n* メッセージストリーム ID (4 bytes)\n  * アプリケーション間接続が完全に成功した際にサーバ側から割り振られる.\n  * チャンクストリームの中ではこの ID を利用して相互に存在を保証しあうため, 一意である必要がある.\n  * **Little Endian**である.\n\nType 1 (7 bytes):\n\n直前のチャンクとメッセージストリーム ID のみが同一である場合はこの Type 1 パターンを入力する.  \n音声・映像チャンク等の可変かつ複数のデータを同時に送信するような場合は, 2 番目に送るチャンクのチャンクメッセージヘッダをこの Type 1 パターンに**すべきである**.\n\n* タイムスタンプ (3 bytes)\n  * Type 0 パターンが送られた時点からのタイムスタンプの**差分**を入力する.\n  * Type 0 パターンと同様に, 0xFFFFFF より大きくなる場合は拡張タイムスタンプを利用する.\n* メッセージ長 (3 bytes)\n  * Type 0 パターンと同様である.\n* メッセージ種類 ID (1 byte)\n  * Type 0 パターンと同様である.\n\nType 2 (3 bytes):\n\n直前のチャンクとメッセージストリーム ID, チャンクデータの種類およびチャンクデータのメッセージ長が同一である場合はこの Type 2 パターンを入力する.  \n固定長の同一の種類のチャンクデータを同じメッセージストリームに送信し続けるような場合は, 2 番目に送るチャンクのチャンクメッセージヘッダをこの Type 2 パターンに**すべきである**.\n\n* タイムスタンプ (3 bytes)\n  * Type 1 パターンと同様の**差分**である.\n  * タイムスタンプが 0xFFFFFF より大きくなる場合も Type 1 と同様にする.\n\nType 3 (0 byte):\n\nこの Type 3 パターンを入力する時は, 以下の 2 つの場合がある:\n\n* 同一のメッセージストリームに種類もサイズも同一のチャンクデータを同時に送信する場合.\n  * チャンクデータの内容まで同一である必要はない.\n* チャンクデータが所定のチャンクサイズより大きくなってしまった場合.\n  * 所定のチャンクサイズ分のチャンクデータの直後に入力する.\n\n特に後者で扱う場合は注意が必要である. その理由は以下の通りである:\n\n* チャンクメッセージヘッダのメッセージ長フィールドにおいて, クライアント側もサーバ側もチャンクデータを区切っている Type 3 ヘッダの数は考慮されて**いない**ため.\n  * つまり, チャンクメッセージヘッダのメッセージ長フィールドをチャンクデータを読み取るための数としてそのまま使おうとすると, **チャンクデータの総量が所定のチャンクサイズを超えている場合に正しく読み取れない**.\n* また, サーバ側もクライアント側もチャンクメッセージヘッダのメッセージ長フィールドの値とは別に Type 3 パターンのチャンクメッセージヘッダで区切られているチャンクデータを繋げる処理を独自に実装してしまっている.\n  * 送信時に入力する Type 3 パターンのチャンクメッセージヘッダの数を当該フィールドに含めても**エラー**扱いされてしまう.\n\n上記の解決手段については別記事で紹介する.\n\n3. 拡張タイムスタンプ (4 bytes)\n\n入力するタイムスタンプが 0xFFFFFF より大きくなった場合に, そのタイムスタンプをチャンクメッセージヘッダのタイムスタンプフィールドに入力する代わりに当該フィールドに入力する.  \nタイムスタンプを拡張する必要がない場合はこのフィールドは入力されないため, 無視してチャンクデータを読むように実装する必要もある.\n\n4. チャンクデータ (可変)\n\nチャンクの本文である. 内容はチャンクメッセージヘッダのメッセージ種類 ID フィールドおよびメッセージ長フィールドの値に依存しているほか, 以下の点にも気をつけなければならない.\n\n* メッセージの種類が同じであっても, チャンクデータの内容も同じであるとは限らない.\n* チャンクデータの長さが所定のチャンクサイズより大きくなる場合はチャンクデータをそのチャンクサイズ毎に区切り, 残りの各チャンクデータにチャンクベーシックヘッダおよび Type 3 パターンのチャンクメッセージヘッダを添えてから送信すべきである.\n\n##### メッセージの種類\n\n|メッセージ種類 ID|チャンクデータの種類|サイズ|入力内容|\n|-|-|-|-|\n|1|Chunk Size|4 bytes|チャンク**データ**を一度に受け取るデータ量. (チャンク全体を指していないことに注意)<br>公式ドキュメントでは Set Chunk Size と呼んでいるが, 既存 OSS 製品では Chunk Size と呼ばれているため, ソースコードとの統一性のために本稿でも Chunk Size と呼ぶことにする.<br>最上位ビットは 0 で**なければならない**.<br>4 bytes が確保されているが実際のチャンクデータの長さの値は高々 3 bytes であるため, 0xFFFFFF よりも大きくなることはまずない.<br>仕様書では少なくとも 128 (bytes) である**べき**で, かつ少なくとも 1 (byte) で**なければならない**としている.<br>一方でデフォルト値を 128 (bytes) としており, 多くの製品はこれに従っている.|\n|2|Abort|4 bytes|送受信を中止する対象のチャンクストリーム ID.<br>何らかの理由でチャンクストリームを強制的に閉じなければならない時に当該チャンクデータにチャンクストリーム ID を入力して終了を伝える.|\n|3|Bytes Read|4 bytes|これまでに受信したデータ量.<br>公式ドキュメントでは Acknowledgement と呼んでいるが既存 OSS 製品では Bytes Read と呼ばれているため, ソースコードとの統一性のために本稿でも Bytes Read と呼ぶことにする.<br>サーバ側もクライアント側も, 受信したデータ量が事前に通知しているウィンドウサイズに等しくなった場合に当該チャンクデータにそのデータ量を入力して送信し**なければならない**.<br>ウィンドウサイズは相手側から当該チャンクデータを受信せずに送れるデータ量の最大値である.|\n|4|User Control|2 bytes<br>+<br>4 bytes から 8 bytes|主にメッセージストリーム ID だが, どの種類のイベントを入力するかによって具体的な内容に違いがある.<br>詳細は [User Control Message の種類とデータ](#user-control-message-の種類とデータ)を参照.|\n|5|Window Acknowledgement Size (Official, FFmpeg),<br>Server BandWidth (Red5, OBS)|4 bytes|サーバ側が Acknowledgement チャンクを送信せずに送れる最大のデータ量.<br>つまりサーバ側の回線帯域である.<br>多くの場合, 3 Mbps 前後をデフォルト値とされているが変更可能である.|\n|6|Set Peer BandWidth (Official, FFmpeg),<br>Client BandWidth (Red5, OBS)|4 bytes|クライアント側が Acknowledgement チャンク送信せずに送れる最大のデータ量.<br>つまりクライアント側の回線帯域である.<br>多くの場合, 3 Mbps 前後をデフォルト値とされているがこれも変更可能である.|\n|8|Audio|可変|音声データ.<br>可変長の生のバイト列が入力される.<br>詳細は後日記載.|\n|9|Video|可変|映像データ.<br>以下同上.<br>詳細は後日記載.|\n|15|Data(Official),<br>Notify(FFmpeg, Red5)<br>Info(OBS)|可変|チャンク(主に映像・音声)のメタデータ.<br>AMF3 がチャンクデータに適用されている.|\n|18|^^|^^|〃<br>AMF0 がチャンクデータに適用されている.|\n|16|Shared Object|可変|名前と値のペアのコレクション.<br>複数のクライアント間やインスタンス間で同期をとるための Flash Objectである.<br>既存の OSS 製品では Red5 のみが実装しているが, 具体的なデータ構造を特定できないため詳細は割愛する.<br>AMF3 がチャンクデータに適用されている.|\n|19|^^|^^|〃<br>AMF0 がチャンクデータに適用されている.|\n|17|Invoke|可変|クライアントとサーバの間で映像の送受信の際に必要になるメッセージを入力する.<br>公式ドキュメントでは Command と呼んでいるが既存 OSS 製品では Invoke と呼ばれているため, ソースコードとの統一性のために本稿でも Invoke と呼ぶことにする.<br>映像・音声データの送受信より前に送受信される基本的なメッセージはすべてこの Invoke チャンクを介して行われる.<br>AMF3 がチャンクデータに適用されている.|\n|20|^^|^^|〃<br>AMF0 がチャンクデータに適用されている.|\n|22|Metadata|可変|音声や映像に関するメタデータ.<br>公式ドキュメントでは Aggregate と呼んでいるが既存 OSS 製品では Metadata と呼ばれているため, ソースコードとの統一性のために本稿でも MetaData と呼ぶことにする.<br>詳細は [Metadata の構造](#metadata-の構造)を参照.|\n\n##### User Control Message の種類とデータ\n\n以下は公式ドキュメントに記載されており, 既存 OSS 製品の実装にも見られるイベントである.\n\n|ID|イベントの種類|サイズ|入力内容|\n|-|-|-|-|\n|0|Stream Begin|4 bytes|クライアントに割り当てられているメッセージストリーム ID.<br>クライアント側からの Invoke(connect) の受信直後は通信の仕様上必然的に 0 になる.|\n|1|Stream EOF|4 bytes|〃<br>プレイバックが終了したクライアントのメッセージストリーム ID を入力する.|\n|2|Stream Dry|4 bytes|〃<br>一定時間以上ストリーム上にデータがないクライアントのメッセージストリーム ID を入力する.|\n|3|Set Buffer Length|8 bytes|クライアントに割り当てられているメッセージストリームID (4 bytes) とミリ秒単位のバッファの長さ (4 bytes).<br>クライアント側がストリームを渡来するデータをバッファリングするために使われるバッファのサイズをサーバ側に通知する.<br>サーバ側がストリームを処理し始める前に送信される.|\n|4|Stream Is Recorded|4 bytes|クライアントに割り当てられているメッセージストリーム ID.<br>サーバ側が当該ストリームが**録画用**として使われていることをクライアント側に通知する.|\n|6|Ping|4 bytes|**サーバ側**のタイムスタンプ.<br>公式ドキュメントでは Ping Request と呼んでいるが既存の OSS 製品では Ping と呼ばれているため, ソースコードとの統一性のために本稿でも Ping と呼ぶことにする.<br>サーバ側が通信がクライアントに到達するかどうかを試すために送信する.|\n|7|Pong|4 bytes|**クライアント側が Ping と共に受け取った**タイムスタンプ.<br>公式ドキュメントでは Ping Response と呼んでいるが既存の OSS 製品では Pong と呼ばれているため, ソースコードとの統一性のために本稿でも Pong と呼ぶことにする.<br>クライアント側がサーバ側からの Ping が到達したことをサーバ側に伝えるために送信する.|\n\n以下は公式ドキュメントには記載されていないが, 既存 OSS 製品の実装で見られるイベントである.\n\n|ID|イベントの種類|サイズ|入力内容|\n|-|-|-|-|\n|26|SWF Verification Request|0 byte|相手側に SWF の内容が正しいことを確かめてもらうためのリクエスト.|\n|27|SWF Verification Response|42 bytes|相手側から返される SWF のバイト列から生成された HMAC-SHA256 ダイジェスト.<br>メッセージの内訳は以下の通りである:  \\\n|||| * 0 byte目: 1  \\\n|||| * 1 byte目: 1  \\\n|||| * 2 - 5 bytes目: 解凍された SWF のサイズ  \\\n|||| * 6 - 9 bytes目: 同上  \\\n|||| * 10 - 31 bytes目: 解凍された SWF のハッシュをハンドシェイクチャンクのダイジェストで署名したバイト列|\n\n以下は公式ドキュメントには記載されておらず, Red5 と OBS の実装で見られるイベントである.\n\n|ID|イベントの種類|サイズ|入力内容|\n|-|-|-|-|\n|31|Buffer Empty|4 bytes|クライアントに割り当てられているメッセージストリーム ID.<br>rtmpdump などの一部のプログラムはバッファのサイズをできるだけ大きく設定し, サーバ側にできるだけ高速にデータを送信させる.<br>サーバ側が完全なバッファをそのようなクライアント側へ送信した際に, バッファを完全に送信し現在のバッファは空の状態であることをクライアント側へ伝えるためにこのイベントを送信する.<br>その後, サーバ側はクライアント側がそのバッファを消費しきるまで送信を待つ.|\n|32|Buffer Ready (OBS),<br>Buffer Full (Red5)|4 bytes|クライアントに割り当てられているメッセージストリーム ID.<br>サーバ側がバッファを送信する準備が出来たことをクライアント側に伝えるためにこのイベントを送信する.|\n\n##### Metadata の構造\n\nMetadata は公式ドキュメント[^RTMP-Specification-1.0]では以下のように定義されている.\n\n> An aggregate message is a single message that contains a series of RTMP sub-messages.\n\n* 集約(Metadata)メッセージは一連の RTMP サブメッセージを含む単一のメッセージである.\n\nそしてそのサブメッセージの内訳は以下のように定義されている.\n\n1. ヘッダ\n2. メッセージデータ\n3. バックポインタ\n\n一方で, 各種 OSS 製品では以下のようにデコードしている.\n\nFFmpeg/rtmpproto.c#L2347-L2395[^FFmpeg/rtmpproto.c#L2347-L2395]\n\n```c\nstatic int handle_metadata(RTMPContext *rt, RTMPPacket *pkt)\n{\n    int ret, old_flv_size, type;\n    const uint8_t *next;\n    uint8_t *p;\n    uint32_t size;\n    uint32_t ts, cts, pts = 0;\n\n    old_flv_size = update_offset(rt, pkt->size);\n\n    if ((ret = av_reallocp(&rt->flv_data, rt->flv_size)) < 0) {\n        rt->flv_size = rt->flv_off = 0;\n        return ret;\n    }\n\n    next = pkt->data;\n    p    = rt->flv_data + old_flv_size;\n\n    /* copy data while rewriting timestamps */\n    ts = pkt->timestamp;\n\n    while (next - pkt->data < pkt->size - RTMP_HEADER) {\n        type = bytestream_get_byte(&next);\n        size = bytestream_get_be24(&next);\n        cts  = bytestream_get_be24(&next);\n        cts |= bytestream_get_byte(&next) << 24;\n        if (!pts)\n            pts = cts;\n        ts += cts - pts;\n        pts = cts;\n        if (size + 3 + 4 > pkt->data + pkt->size - next)\n            break;\n        bytestream_put_byte(&p, type);\n        bytestream_put_be24(&p, size);\n        bytestream_put_be24(&p, ts);\n        bytestream_put_byte(&p, ts >> 24);\n        memcpy(p, next, size + 3 + 4);\n        p    += size + 3;\n        bytestream_put_be32(&p, size + RTMP_HEADER);\n        next += size + 3 + 4;\n    }\n    if (p != rt->flv_data + rt->flv_size) {\n        av_log(NULL, AV_LOG_WARNING, "Incomplete flv packets in "\n                                     "RTMP_PT_METADATA packet\\n");\n        rt->flv_size = p - rt->flv_data;\n    }\n\n    return 0;\n}\n```\n\nobs-studio/rtmp.c#L1490-L1523[^obs-studio/rtmp.c#L1490-L1523]\nobs-studio/rtmp.c#L4972-L5059[^obs-studio/rtmp.c#L4972-L5059]\n\n```c\ncase RTMP_PACKET_TYPE_FLASH_VIDEO:\n{\n    /* go through FLV packets and handle metadata packets */\n    unsigned int pos = 0;\n    uint32_t nTimeStamp = packet->m_nTimeStamp;\n\n    while (pos + 11 < packet->m_nBodySize)\n    {\n        uint32_t dataSize = AMF_DecodeInt24(packet->m_body + pos + 1);\t/* size without header (11) and prevTagSize (4) */\n\n        if (pos + 11 + dataSize + 4 > packet->m_nBodySize)\n        {\n            RTMP_Log(RTMP_LOGWARNING, "Stream corrupt?!");\n            break;\n        }\n        if (packet->m_body[pos] == 0x12)\n        {\n            HandleMetadata(r, packet->m_body + pos + 11, dataSize);\n        }\n        else if (packet->m_body[pos] == 8 || packet->m_body[pos] == 9)\n        {\n            nTimeStamp = AMF_DecodeInt24(packet->m_body + pos + 4);\n            nTimeStamp |= (packet->m_body[pos + 7] << 24);\n        }\n        pos += (11 + dataSize + 4);\n    }\n    if (!r->m_pausing)\n        r->m_mediaStamp = nTimeStamp;\n\n    /* FLV tag(s) */\n    /*RTMP_Log(RTMP_LOGDEBUG, "%s, received: FLV tag(s) %lu bytes", __FUNCTION__, packet.m_nBodySize); */\n    bHasMediaPacket = 1;\n    break;\n}\n\n// 中略\n\nif (packet.m_packetType == RTMP_PACKET_TYPE_FLASH_VIDEO)\n{\n    /* basically we have to find the keyframe with the\n     * correct TS being nResumeTS\n     */\n    unsigned int pos = 0;\n    uint32_t ts = 0;\n\n    while (pos + 11 < nPacketLen)\n    {\n        /* size without header (11) and prevTagSize (4) */\n        uint32_t dataSize =\n            AMF_DecodeInt24(packetBody + pos + 1);\n        ts = AMF_DecodeInt24(packetBody + pos + 4);\n        ts |= (packetBody[pos + 7] << 24);\n\n#ifdef _DEBUG\n        RTMP_Log(RTMP_LOGDEBUG,\n                 "keyframe search: FLV Packet: type %02X, dataSize: %d, timeStamp: %d ms",\n                 packetBody[pos], dataSize, ts);\n#endif\n        /* ok, is it a keyframe?:\n         * well doesn\'t work for audio!\n         */\n        if (packetBody[pos /*6928, test 0 */ ] ==\n                r->m_read.initialFrameType\n                /* && (packetBody[11]&0xf0) == 0x10 */ )\n        {\n            if (ts == r->m_read.nResumeTS)\n            {\n                RTMP_Log(RTMP_LOGDEBUG,\n                         "Found keyframe with resume-keyframe timestamp!");\n                if (r->m_read.nInitialFrameSize != dataSize\n                        || memcmp(r->m_read.initialFrame,\n                                  packetBody + pos + 11,\n                                  r->m_read.\n                                  nInitialFrameSize) != 0)\n                {\n                    RTMP_Log(RTMP_LOGERROR,\n                             "FLV Stream: Keyframe doesn\'t match!");\n                    ret = RTMP_READ_ERROR;\n                    break;\n                }\n                r->m_read.flags |= RTMP_READ_GOTFLVK;\n\n                /* skip this packet?\n                 * check whether skippable:\n                 */\n                if (pos + 11 + dataSize + 4 > nPacketLen)\n                {\n                    RTMP_Log(RTMP_LOGWARNING,\n                             "Non skipable packet since it doesn\'t end with chunk, stream corrupt!");\n                    ret = RTMP_READ_ERROR;\n                    break;\n                }\n                packetBody += (pos + 11 + dataSize + 4);\n                nPacketLen -= (pos + 11 + dataSize + 4);\n\n                goto stopKeyframeSearch;\n\n            }\n            else if (r->m_read.nResumeTS < ts)\n            {\n                /* the timestamp ts will only increase with\n                 * further packets, wait for seek\n                 */\n                goto stopKeyframeSearch;\n            }\n        }\n        pos += (11 + dataSize + 4);\n    }\n    if (ts < r->m_read.nResumeTS)\n    {\n        RTMP_Log(RTMP_LOGERROR,\n                 "First packet does not contain keyframe, all "\n                 "timestamps are smaller than the keyframe "\n                 "timestamp; probably the resume seek failed?");\n    }\nstopKeyframeSearch:\n    ;\n    if (!(r->m_read.flags & RTMP_READ_GOTFLVK))\n    {\n        RTMP_Log(RTMP_LOGERROR,\n                 "Couldn\'t find the seeked keyframe in this chunk!");\n        ret = RTMP_READ_IGNORE;\n        break;\n    }\n}\n```\n\nred5-server-common/Aggregate.java#L119-L209[^red5-server-common/Aggregate.java#L119-L209]\n\n```java\n/**\n * Breaks-up the aggregate into its individual parts and returns them as a list. The parts are returned based on the ordering of the aggregate itself.\n * \n * @return list of IRTMPEvent objects\n */\npublic LinkedList<IRTMPEvent> getParts() {\n    LinkedList<IRTMPEvent> parts = new LinkedList<IRTMPEvent>();\n    log.trace("Aggregate data length: {}", data.limit());\n    int position = data.position();\n    do {\n        try {\n            // read the header\n            //log.trace("Hex: {}", data.getHexDump());\n            byte subType = data.get();\n            // when we run into subtype 0 break out of here\n            if (subType == 0) {\n                log.debug("Subtype 0 encountered within this aggregate, processing with exit");\n                break;\n            }\n            int size = IOUtils.readUnsignedMediumInt(data);\n            log.debug("Data subtype: {} size: {}", subType, size);\n            // TODO ensure the data contains all the bytes to support the specified size\n            int timestamp = IOUtils.readExtendedMediumInt(data);\n            /*timestamp = ntohap((GETIBPOINTER(buffer) + 4)); 0x12345678 == 34 56 78 12*/\n            int streamId = IOUtils.readUnsignedMediumInt(data);\n            log.debug("Data timestamp: {} stream id: {}", timestamp, streamId);\n            Header partHeader = new Header();\n            partHeader.setChannelId(header.getChannelId());\n            partHeader.setDataType(subType);\n            partHeader.setSize(size);\n            // use the stream id from the aggregate\'s header\n            partHeader.setStreamId(header.getStreamId());\n            partHeader.setTimer(timestamp);\n            // timer delta == time stamp - timer base\n            // the back pointer may be used to verify the size of the individual part\n            // it will be equal to the data size + header size\n            int backPointer = 0;\n            switch (subType) {\n                case TYPE_AUDIO_DATA:\n                    AudioData audio = new AudioData(data.getSlice(size));\n                    audio.setTimestamp(timestamp);\n                    audio.setHeader(partHeader);\n                    log.debug("Audio header: {}", audio.getHeader());\n                    parts.add(audio);\n                    //log.trace("Hex: {}", data.getHexDump());\n                    // ensure 4 bytes left to read an int\n                    if (data.position() < data.limit() - 4) {\n                        backPointer = data.getInt();\n                        //log.trace("Back pointer: {}", backPointer);\n                        if (backPointer != (size + 11)) {\n                            log.debug("Data size ({}) and back pointer ({}) did not match", size, backPointer);\n                        }\n                    }\n                    break;\n                case TYPE_VIDEO_DATA:\n                    VideoData video = new VideoData(data.getSlice(size));\n                    video.setTimestamp(timestamp);\n                    video.setHeader(partHeader);\n                    log.debug("Video header: {}", video.getHeader());\n                    parts.add(video);\n                    //log.trace("Hex: {}", data.getHexDump());\n                    // ensure 4 bytes left to read an int\n                    if (data.position() < data.limit() - 4) {\n                        backPointer = data.getInt();\n                        //log.trace("Back pointer: {}", backPointer);\n                        if (backPointer != (size + 11)) {\n                            log.debug("Data size ({}) and back pointer ({}) did not match", size, backPointer);\n                        }\n                    }\n                    break;\n                default:\n                    log.debug("Non-A/V subtype: {}", subType);\n                    Unknown unk = new Unknown(subType, data.getSlice(size));\n                    unk.setTimestamp(timestamp);\n                    unk.setHeader(partHeader);\n                    parts.add(unk);\n                    // ensure 4 bytes left to read an int\n                    if (data.position() < data.limit() - 4) {\n                        backPointer = data.getInt();\n                    }\n            }\n            position = data.position();\n        } catch (Exception e) {\n            log.error("Exception decoding aggregate parts", e);\n            break;\n        }\n        log.trace("Data position: {}", position);\n    } while (position < data.limit());\n    log.trace("Aggregate processing complete, {} parts extracted", parts.size());\n    return parts;\n}\n```\n\n上記の各実装において, `type/subType`, `size/dataSize` および `cts/nTimestamp/timestamp` として表れているフィールドは公式ドキュメント[^RTMP-Specification-1.0]中の以下の部分で定義されている.\n\n> 6.1.1.  Message Header\n>\n> The message header contains the following:\n>\n> Message Type: One byte field to represent the message type. A range of type IDs (1-6) are reserved for protocol control messages.  \n> Length: Three-byte field that represents the size of the payload in bytes. It is set in big-endian format.  \n> Timestamp: Four-byte field that contains a timestamp of the message. The 4 bytes are packed in the big-endian order.  \n> Message Stream Id: Three-byte field that identifies the stream of the message. These bytes are set in big-endian format.\n\n1. メッセージの種類 (ID, 1 byte)\n\n* 1 - 6 はプロトコル制御メッセージ用に予約されている.\n\n2. 長さ(3 bytes)\n\n* Big Endianである.\n\n3. タイムスタンプ (4 bytes)\n\n* Big Endianである. と書かれているが, 上記の各実装では以下のデコードを行っている:\n  * チャンク中の最下位の 1 byte を実際のタイムスタンプの最上位 1 byte とする.\n  * 実際のタイムスタンプからチャンクにエンコードする場合は, 上記の逆の操作を行う.\n\n4. メッセージストリーム ID (3 bytes)\n\n* Big Endian である.\n\nメッセージストリーム ID について:\n\n> The message stream ID of the aggregate message overrides the message stream IDs of the sub-messages inside the aggregate.\n\n* 集約メッセージのチャンクに割り当てられているメッセージストリーム ID はサブメッセージに割り当てられているメッセージストリーム ID を無視する.\n\nタイムスタンプについて:\n\n> The difference between the timestamps of the aggregate message and the first sub-message is the offset used to renormalize the timestamps of the sub-messages to the stream timescale.\n> The offset is added to each sub-message’s timestamp to arrive at the normalized stream time.\n> The timestamp of the first sub-message SHOULD be the same as the timestamp of the aggregate message, so the offset SHOULD be zero.\n\n* サブメッセージのタイムスタンプはチャンクメッセージヘッダに入力されたタイムスタンプを基準にしたオフセットである.\n* チャンクメッセージヘッダのタイムスタンプに当該フィールドの値を加算することで実際のタイムスタンプを求めることができる.\n* 最初のサブメッセージのタイムスタンプはチャンクメッセージヘッダのそれと同一で**あるべき**なので, オフセットは 0 で**あるべき**である.\n\nそして, バックポインタについては公式ドキュメント[^RTMP-Specification-1.0]では以下のように定義されている.\n\n> The back pointer contains the size of the previous message including its header.\n> It is included to match the format of FLV file and is used for backward seek.\n\n* サブヘッダを含む直前のサブメッセージのサイズである.\n* FLV ファイルのフォーマットに一致しており, 逆シークに使われる.\n\n当該サブヘッダに入力するメッセージの種類は上記の各実装を参考にすると以下のようである.\n\n|ID|サブメッセージの種類|入力内容|\n|-|-|-|\n|8|Audio|音声データ.<br>生のバイト列である.|\n|9|Video|映像データ.<br>〃|\n|18|Data(Official),<br>Notify(FFmpeg, Red5),<br>Info(OBS)|サブメッセージのメタデータ.<br>AMF0 がサブメッセージに適用されている.|\n\n上記の FFmpeg および OBS の実装に着目すると, いずれもメッセージストリーム ID に相当するフィールドを意図的にデコードして**いない**ことを確認できる. 同様に, バックポインタの値もデコードして**いない**ことを確認できる.  \nところで, 上記ソースコード中に `1`, `3`, `4`, `7` および `11` といったマジックナンバーが散見される. これらは以下の計算に用いられている.\n\nFFmpeg:\n\n* 3: メッセージストリーム ID のサイズ. デコードはしないがそのままコピーして使い回すため, コピーするサイズをその分だけ加算している.\n* 4: バックポインタのサイズ. 受信したチャンクに入力されている分に関してはデコードしないが, 送信するチャンクには入力が必須なため, サイズをその分だけ加算している.\n* RTMP\\_HEADER: 11. つまり Metadata チャンクに入力されているサブヘッダ全体のサイズである.\n\nOBS:\n\n* 1: サブメッセージの種類 (ID). OBS ではメッセージストリーム ID を読み飛ばしている箇所があるため, その分だけオフセットしている.\n* 4: タイムスタンプの開始位置. サブメッセージの種類を表す ID (1 byte) と サブメッセージの長さ (3 bytes) を合計した分だけオフセットしている.\n* 7: タイムスタンプの **4 bytes目**の位置. タイムスタンプの開始位置にタイムスタンプの上位 3 bytes のサイズをさらに合計した分だけオフセットしている.\n* 11: Metadata チャンクに入力されているサブヘッダのサイズ.\n\n#### Invoke(connect)\n\nInvoke(connect) およびその応答メッセージは公式ドキュメント[^RTMP-Specification-1.0]では以下のように定義されている.\n\n要求メッセージ:\n\n|フィールド名|データ型|入力内容|\n|-|-|-|\n|コマンド名|String|connect|\n|トランザクション ID|Number|1|\n|コマンドオブジェクト|Object|名前と値のペア.<br>クライアント側のアプリケーションを接続するために必要な情報が書き込まれている.|\n|追加のユーザ引数|Object|コマンドオブジェクトの他に必要な情報がある場合に入力する.|\n\n応答メッセージ:\n\n|フィールド名|データ型|入力内容|\n|-|-|-|\n|コマンド名|String|\\_result(アプリケーションを接続できる時)<br>もしくは<br>\\_error(アプリケーションを接続できない時)|\n|トランザクション ID|Number|1|\n|プロパティ|Object|名前と値のペア.<br>サーバ側のアプリケーションを接続するために必要な情報を入力する.|\n|インフォメーション|Object|名前と値のペア.<br>サーバ側の応答の状態を表すために必要な情報を入力する.|\n\nコマンドオブジェクト:\n\n|プロパティ|データ型|入力内容|\n|-|-|-|\n|app|String|クライアントが接続しているサーバアプリケーションの名前.<br>多くの場合において, 起動時に渡される URL から参照する.<br>そのパターンは次の通りである: protocol://server[:port][/app][/playpath]|\n|type|String|nonprivate.<br>公式ドキュメントには定義されていないが FFmpeg や OBS で入力されている.|\n|flashVer|String|Flash Player のバージョン.<br>入力側と出力側で入力内容が違う.<br>出力側の場合: FMLE/3.0 (compatible; &lt;クライアント側のツールやライブラリの識別情報&gt;)<br>入力側の場合: &lt;OSの識別名&gt; &lt;Flash Playerのバージョン(カンマ区切り)&gt;|\n|swfUrl|String|アプリケーション接続に必要な SWF ファイルの URL.<br>ツールによってデフォルトの入力内容に違いがある. 例えば:<br>FFmpeg の場合: 入力なし.<br>OBS の場合: tcUrl と同じ値.|\n|tcUrl|String|接続先サーバの URL.<br>protocol://server[:port][/app] のフォーマットに従って入力する.<br>デフォルトは起動時にコマンドラインで渡された URL を参照する.|\n|fpad|Boolean|プロキシが使われているなら true を入力する.|\n|capabilities|Number|15. 公式ドキュメントには定義されていないが FFmpeg や OBS では入力されている.|\n|audioCodecs|Number|クライアントがサポートする音声コーデックの情報.|\n|videoCodecs|Number|クライアントがサポートする映像コーデックの情報.|\n|videoFunction|Number|クライアントがサポートする特別なビデオ機能の情報.|\n|pageUrl|String|SWF ファイルがロードされた Web ページの URL.|\n|objectEncoding|Number|AMF のエンコーディングメソッド.|\n\nサポートしている音声コーデック:\n\n|ビットフラグ|コーデック|備考|\n|-|-|-|\n|0x0001|Raw| |\n|0x0002|ADPCM| |\n|0x0004|MP3| |\n|0x0008|Intel|使われていない.|\n|0x0010|Unused|使われていない.|\n|0x0020|Nerry8|NellyMoser at 8 kHz.|\n|0x0040|Nerry|NellyMoser at 5, 11, 22 and 44 kHz.|\n|0x0080|G711A|Adobe Media Server 限定のコーデックである.|\n|0x0100|G711U|同上.|\n|0x0200|NELLY16|NellyMouser at 16 kHz.|\n|0x0400|AAC| |\n|0x0800|Speex| |\n|0xFFFF|上記のすべて| |\n\nサポートしている映像コーデック:\n\n|ビットフラグ|コーデック|備考|\n|-|-|-|\n|0x0001|Unused|廃れている.|\n|0x0002|JPEG|廃れている.|\n|0x0004|Sorenson| |\n|0x0008|Homebrew| |\n|0x0010|On2VP6|Flash 8 以降にサポートしている.|\n|0x0020|On2VP6 with alpha channel|同上.|\n|0x0040|Homebrew v2| |\n|0x0080|H264| |\n|0x00FF|上記のすべて| |\n\nサポートしているビデオ機能:\n\n|ビットフラグ|機能|備考|\n|-|-|-|\n|1|Seek|クライアント側はフレーム精度の高いシークを実行できる.|\n\nサポートしているエンコーディングメソッド:\n\n|ビットフラグ|エンコーディング|備考|\n|-|-|-|\n|0|AMF0|Flash 6 以降にサポートしている.|\n|3|AMF3|Flash 9 (ActionScript 3) 以降にサポートしている.|\n\n応答メッセージのプロパティフィールドおよびインフォメーションフィールドには公式に定められた仕様が存在しない. よって, 各種 OSS 製品の実装内容から特定できる範囲で紹介する.\n\nFFmpeg/rtmpproto.c#L542-L575[^FFmpeg/rtmpproto.c#L542-L575]\n\n```c\n// Send _result NetConnection.Connect.Success to connect\nif ((ret = ff_rtmp_packet_create(&pkt, RTMP_SYSTEM_CHANNEL,\n                                 RTMP_PT_INVOKE, 0,\n                                 RTMP_PKTDATA_DEFAULT_SIZE)) < 0)\n    return ret;\n\np = pkt.data;\nff_amf_write_string(&p, "_result");\nff_amf_write_number(&p, seqnum);\n\nff_amf_write_object_start(&p);\nff_amf_write_field_name(&p, "fmsVer");\nff_amf_write_string(&p, "FMS/3,0,1,123");\nff_amf_write_field_name(&p, "capabilities");\nff_amf_write_number(&p, 31);\nff_amf_write_object_end(&p);\n\nff_amf_write_object_start(&p);\nff_amf_write_field_name(&p, "level");\nff_amf_write_string(&p, "status");\nff_amf_write_field_name(&p, "code");\nff_amf_write_string(&p, "NetConnection.Connect.Success");\nff_amf_write_field_name(&p, "description");\nff_amf_write_string(&p, "Connection succeeded.");\nff_amf_write_field_name(&p, "objectEncoding");\nff_amf_write_number(&p, 0);\nff_amf_write_object_end(&p);\n\npkt.size = p - pkt.data;\nret = ff_rtmp_packet_write(rt->stream, &pkt, rt->out_chunk_size,\n                           &rt->prev_pkt[1], &rt->nb_prev_pkt[1]);\nff_rtmp_packet_destroy(&pkt);\nif (ret < 0)\n    return ret;\n```\n\nプロパティ:\n\n|プロパティ|データ型|入力内容|\n|-|-|-|\n|fmsVer|String|FMS/&lt;Adobe Media Serverのバージョン(カンマ区切り)&gt;|\n|capabilities|Number|31(暫定)|\n\nインフォメーション:\n\n|プロパティ|データ型|入力内容|\n|-|-|-|\n|level|String|status|\n|code|String|NetConnection.Connect.Success|\n|description|String|Connection succeeded.|\n|objectEncoding|Number|0|\n\nそして, 当該チャンクの送受信の手順は公式ドキュメント[^RTMP-Specification-1.0]では以下のように定義されている.\n\n<div id="rtmp-invoke-connect-sequences-official">\n\n@startuml\n== RTMP ハンドシェイクが完了した. ==\nClient -> Server: Invoke(connect)\nServer -> Client: Window Acknowledgement Size / Server BandWidth\nServer -> Client: Set Peer BandWidth / Client BandWidth\nClient -> Server: Window Acknowledgement Size / Server BandWidth\nServer -> Client: User Control(Stream Begin)\nServer -> Client: Invoke(_result)\n@enduml\n\n</div>\n\n図5. 公式が説明している Invoke(connect) のシーケンス {#caption-rtmp-invoke-connect-sequences-official}\n\n> The message flow during the execution of the command is:\n>\n> 1. Client sends the connect command to the server to request to connect with the server application instance.\n> 2. After receiving the connect command, the server sends the protocol message ’Window Acknowledgement Size’ to the client. The server also connects to the application mentioned in the connect command.\n> 3. The server sends the protocol message ’Set Peer Bandwidth’ to the client.\n> 4. The client sends the protocol message ’Window Acknowledgement Size’ to the server after processing the protocol message ’Set Peer Bandwidth’.\n> 5. The server sends an another protocol message of type User Control Message(StreamBegin) to the client.\n> 6. The server sends the result command message informing the client of the connection status (success/fail). The command specifies the transaction ID (always equal to 1 for the connect command). The message also specifies the properties, such as Flash Media Server version (string). In addition it specificies other connection response related information like level (string), code (string), description (string), objectencoding (number), etc.\n\n* クライアント側はサーバ側のアプリケーションとの接続を要求するために, サーバ側に Invoke(connect) を送信する.\n* Invoke(connect) の受信後, サーバ側は プロトコルメッセージ Window Acknowledgement Size / Client BandWidth をクライアント側に送信する. サーバ側もまた connect コマンドで指定されたアプリケーションに接続する.\n* サーバ側はクライアント側にプロトコルメッセージ Set Peer BandWidth / Client BandWidth をクライアント側に送信する.\n* Set Peer BandWidth / Client BandWidth の処理後に, クライアント側はサーバ側にプロトコルメッセージ Window Acknowledgement Size / Server BandWidth を送信する.\n* サーバ側はクライアント側に他のプロトコルメッセージである User Control(Stream Begin) を送信する.\n* サーバ側はクライアント側にクライアント側の接続状態を通知する Invoke(\\_result) を送信する.\n\n以下に FFmpeg が実際に送信しているメッセージを示す.\n\nFFmpeg/rtmpproto.c#L485-L588[^FFmpeg/rtmpproto.c#L485-L588]\n\n```c\n// Send Window Acknowledgement Size (as defined in specification)\nif ((ret = ff_rtmp_packet_create(&pkt, RTMP_NETWORK_CHANNEL,\n                                 RTMP_PT_WINDOW_ACK_SIZE, 0, 4)) < 0)\n    return ret;\np = pkt.data;\n// Inform the peer about how often we want acknowledgements about what\n// we send. (We don\'t check for the acknowledgements currently.)\nbytestream_put_be32(&p, rt->max_sent_unacked);\npkt.size = p - pkt.data;\nret = ff_rtmp_packet_write(rt->stream, &pkt, rt->out_chunk_size,\n                           &rt->prev_pkt[1], &rt->nb_prev_pkt[1]);\nff_rtmp_packet_destroy(&pkt);\nif (ret < 0)\n    return ret;\n// Set Peer Bandwidth\nif ((ret = ff_rtmp_packet_create(&pkt, RTMP_NETWORK_CHANNEL,\n                                 RTMP_PT_SET_PEER_BW, 0, 5)) < 0)\n    return ret;\np = pkt.data;\n// Tell the peer to only send this many bytes unless it gets acknowledgements.\n// This could be any arbitrary value we want here.\nbytestream_put_be32(&p, rt->max_sent_unacked);\nbytestream_put_byte(&p, 2); // dynamic\npkt.size = p - pkt.data;\nret = ff_rtmp_packet_write(rt->stream, &pkt, rt->out_chunk_size,\n                           &rt->prev_pkt[1], &rt->nb_prev_pkt[1]);\nff_rtmp_packet_destroy(&pkt);\nif (ret < 0)\n    return ret;\n\n// User control\nif ((ret = ff_rtmp_packet_create(&pkt, RTMP_NETWORK_CHANNEL,\n                                 RTMP_PT_USER_CONTROL, 0, 6)) < 0)\n    return ret;\n\np = pkt.data;\nbytestream_put_be16(&p, 0); // 0 -> Stream Begin\nbytestream_put_be32(&p, 0); // Stream 0\nret = ff_rtmp_packet_write(rt->stream, &pkt, rt->out_chunk_size,\n                           &rt->prev_pkt[1], &rt->nb_prev_pkt[1]);\nff_rtmp_packet_destroy(&pkt);\nif (ret < 0)\n    return ret;\n\n// Chunk size\nif ((ret = ff_rtmp_packet_create(&pkt, RTMP_NETWORK_CHANNEL,\n                                 RTMP_PT_CHUNK_SIZE, 0, 4)) < 0)\n    return ret;\n\np = pkt.data;\nbytestream_put_be32(&p, rt->out_chunk_size);\nret = ff_rtmp_packet_write(rt->stream, &pkt, rt->out_chunk_size,\n                           &rt->prev_pkt[1], &rt->nb_prev_pkt[1]);\nff_rtmp_packet_destroy(&pkt);\nif (ret < 0)\n    return ret;\n\n// Send _result NetConnection.Connect.Success to connect\nif ((ret = ff_rtmp_packet_create(&pkt, RTMP_SYSTEM_CHANNEL,\n                                 RTMP_PT_INVOKE, 0,\n                                 RTMP_PKTDATA_DEFAULT_SIZE)) < 0)\n    return ret;\n\np = pkt.data;\nff_amf_write_string(&p, "_result");\nff_amf_write_number(&p, seqnum);\n\nff_amf_write_object_start(&p);\nff_amf_write_field_name(&p, "fmsVer");\nff_amf_write_string(&p, "FMS/3,0,1,123");\nff_amf_write_field_name(&p, "capabilities");\nff_amf_write_number(&p, 31);\nff_amf_write_object_end(&p);\n\nff_amf_write_object_start(&p);\nff_amf_write_field_name(&p, "level");\nff_amf_write_string(&p, "status");\nff_amf_write_field_name(&p, "code");\nff_amf_write_string(&p, "NetConnection.Connect.Success");\nff_amf_write_field_name(&p, "description");\nff_amf_write_string(&p, "Connection succeeded.");\nff_amf_write_field_name(&p, "objectEncoding");\nff_amf_write_number(&p, 0);\nff_amf_write_object_end(&p);\n\npkt.size = p - pkt.data;\nret = ff_rtmp_packet_write(rt->stream, &pkt, rt->out_chunk_size,\n                           &rt->prev_pkt[1], &rt->nb_prev_pkt[1]);\nff_rtmp_packet_destroy(&pkt);\nif (ret < 0)\n    return ret;\n\nif ((ret = ff_rtmp_packet_create(&pkt, RTMP_SYSTEM_CHANNEL,\n                                 RTMP_PT_INVOKE, 0, 30)) < 0)\n    return ret;\np = pkt.data;\nff_amf_write_string(&p, "onBWDone");\nff_amf_write_number(&p, 0);\nff_amf_write_null(&p);\nff_amf_write_number(&p, 8192);\npkt.size = p - pkt.data;\nret = ff_rtmp_packet_write(rt->stream, &pkt, rt->out_chunk_size,\n                           &rt->prev_pkt[1], &rt->nb_prev_pkt[1]);\nff_rtmp_packet_destroy(&pkt);\n```\n\n<div id="rtmp-invoke-connect-sequences-ffmpeg">\n\n@startuml\n== ハンドシェイクが完了した. ==\nクライアント -> サーバ: Invoke(connect)\nサーバ -> クライアント: Window Acknowledgement Size / Server BandWidth\nサーバ -> クライアント: Set Peer BandWidth / Client BandWidth\nサーバ -> クライアント: User Control(Stream Begin)\nサーバ -> クライアント: Chunk Size\nサーバ -> クライアント: Invoke(_result)\nサーバ -> クライアント: Invoke(onBWDone)\n@enduml\n\n</div>\n\n図6. FFmpeg が行っている Invoke(connect) のシーケンス {#caption-rtmp-invoke-connect-sequences-ffmpeg}\n\n以下の項目はすべてサーバ側からクライアント側への送信として記述する.\n\n1. Window Acknowledgement Size / Server BandWidth を送信する.\n2. Set Peer Bandwidth / Client BandWidth を送信する.\n3. User Control(Stream Begin) を送信する.\n4. **Chunk Size** を送信する.\n5. Invoke(\\_result) を送信する.\n6. **Invoke(onBWDone)** を送信する.\n\n公式ドキュメントが公開された RTMP 1.0 当時と最新の RTMP クライアント/サーバとで手順に変更があることを確認できる. しかし, どちらの手順もアプリケーション接続に**失敗**する.\n\n公式ドキュメントに従った場合:\n\nそもそも公開当時に対して手順が変更されてしまっているため, 当然に失敗してしまう.\n\nFFmpeg に従った場合:\n\nInvoke(onBWDone) を送信した段階で, FFmpeg が以下のメッセージと共にプロセスを終了してしまうはずである.\n\n> RTMP packet size mismatch N != M\n\nここで N は Invoke(onBWDone) チャンクのメッセージ長を, M はその直前に送信した Invoke(\\_result) チャンクのメッセージ長を指している. 上記のエラーメッセージから考えると, Invoke(\\_result) チャンクの受信後にもう一度同じサイズのメッセージを要求している. つまり, 何故か Invoke(\\_result) チャンクを**二度**送信しなければならない.  \nなお, 上記のエラーメッセージは当該製品中の以下の処理から発されている.\n\nFFmpeg/rtmppkt.c#L238-L244[^FFmpeg/rtmppkt.c#L238-L244]\n\n```c\nif (prev_pkt[channel_id].read && size != prev_pkt[channel_id].size) {\n    av_log(h, AV_LOG_ERROR, "RTMP packet size mismatch %d != %d\\n",\n                            size, prev_pkt[channel_id].size);\n    ff_rtmp_packet_destroy(&prev_pkt[channel_id]);\n    prev_pkt[channel_id].read = 0;\n    return AVERROR_INVALIDDATA;\n}\n```\n\n上記の処理は当該製品中のソースコードにしか存在せず, 他方の RTMP クライアントソフトウェアである OBS のソースコード中には存在しないことを確認できる.\n\nobs-studio/rtmp.c#L3857-L4049[^obs-studio/rtmp.c#L3857-L4049]\n\nよって, 私は Invoke(connect) の応答メッセージの送信手順を以下に変更して再送信を試みた.\n\n<div id="rtmp-invoke-connect-sequences-fixed">\n\n@startuml\n== ハンドシェイクが完了した. ==\nクライアント -> サーバ: Invoke(connect)\nサーバ -> クライアント: Invoke(_result)\nサーバ -> クライアント: Window Acknowledgement Size / Server BandWidth\nサーバ -> クライアント: Set Peer BandWidth / Client BandWidth\nサーバ -> クライアント: User Control(Stream Begin)\nサーバ -> クライアント: Chunk Size\nサーバ -> クライアント: Invoke(_result)\n@enduml\n\n</div>\n\n図7. エラーメッセージ対処後の Invoke(connect) のシーケンス {#caption-rtmp-invoke-connect-sequences-fixed}\n\n以下の項目もサーバ側からクライアント側への送信として記述する.\n\n1. **Invoke(\\_result)** を送信する.\n2. Window Acknowledgement Size / Server BandWidth を送信する.\n3. Set Peer Bandwidth / Client BandWidth を送信する.\n4. User Control(Stream Begin) を送信する.\n5. Chunk Size を送信する.\n6. **Invoke(\\_result)** を送信する.\n\nすると上記のエラーメッセージは発されなくなったが, 今度は Invoke(onBWDone) チャンクを送信する前の段階で FFmpeg から新たな要求メッセージを受信した. それ(ら)は Invoke(createStream) チャンクとそれに付随して送信される**新仕様の** Invoke メッセージである.\n\n#### Invoke(releaseStream), Invoke(FCPublish), Invoke(createStream)\n\nInvoke(connect) での接続処理が終わった後に, 3 つに繋がった何らかのチャンクを受信する. それらは Invoke(createStream) と各種製品が公式ドキュメントの公開よりも後に実装した要求メッセージである.\n\n1. Invoke(releaseStream)\n\nInvoke(releaseStream) チャンクとその応答メッセージは FFmpeg および OBS によると以下の構造であるようだ.\n\nFFmpeg/rtmpproto.c#L593-L615[^FFmpeg/rtmpproto.c#L593-L615]\nFFmpeg/rtmpproto.c#L1981-L1999[^FFmpeg/rtmpproto.c#L1981-L1999]\n\n```c\n/**\n * Generate \'releaseStream\' call and send it to the server. It should make\n * the server release some channel for media streams.\n */\nstatic int gen_release_stream(URLContext *s, RTMPContext *rt)\n{\n    RTMPPacket pkt;\n    uint8_t *p;\n    int ret;\n\n    if ((ret = ff_rtmp_packet_create(&pkt, RTMP_SYSTEM_CHANNEL, RTMP_PT_INVOKE,\n                                     0, 29 + strlen(rt->playpath))) < 0)\n        return ret;\n\n    av_log(s, AV_LOG_DEBUG, "Releasing stream...\\n");\n    p = pkt.data;\n    ff_amf_write_string(&p, "releaseStream");\n    ff_amf_write_number(&p, ++rt->nb_invokes);\n    ff_amf_write_null(&p);\n    ff_amf_write_string(&p, rt->playpath);\n\n    return rtmp_send_packet(rt, &pkt, 1);\n}\n\n// 中略\n\nif ((ret = ff_rtmp_packet_create(&spkt, RTMP_SYSTEM_CHANNEL,\n                                 RTMP_PT_INVOKE, 0,\n                                 RTMP_PKTDATA_DEFAULT_SIZE)) < 0) {\n    av_log(s, AV_LOG_ERROR, "Unable to create response packet\\n");\n    return ret;\n}\npp = spkt.data;\nff_amf_write_string(&pp, "_result");\nff_amf_write_number(&pp, seqnum);\nff_amf_write_null(&pp);\nif (!strcmp(command, "createStream")) {\n    rt->nb_streamid++;\n    if (rt->nb_streamid == 0 || rt->nb_streamid == 2)\n        rt->nb_streamid++; /* Values 0 and 2 are reserved */\n    ff_amf_write_number(&pp, rt->nb_streamid);\n    /* By now we don\'t control which streams are removed in\n     * deleteStream. There is no stream creation control\n     * if a client creates more than 2^32 - 2 streams. */\n}\n```\n\nobs-studio/rtmp.c#L1990-L2016[^obs-studio/rtmp.c#L1990-L2016]\n\n```c\nstatic int\nSendReleaseStream(RTMP *r, int streamIdx)\n{\n    RTMPPacket packet;\n    char pbuf[1024], *pend = pbuf + sizeof(pbuf);\n    char *enc;\n\n    packet.m_nChannel = 0x03;\t/* control channel (invoke) */\n    packet.m_headerType = RTMP_PACKET_SIZE_MEDIUM;\n    packet.m_packetType = RTMP_PACKET_TYPE_INVOKE;\n    packet.m_nTimeStamp = 0;\n    packet.m_nInfoField2 = 0;\n    packet.m_hasAbsTimestamp = 0;\n    packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE;\n\n    enc = packet.m_body;\n    enc = AMF_EncodeString(enc, pend, &av_releaseStream);\n    enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes);\n    *enc++ = AMF_NULL;\n    enc = AMF_EncodeString(enc, pend, &r->Link.streams[streamIdx].playpath);\n    if (!enc)\n        return FALSE;\n\n    packet.m_nBodySize = enc - packet.m_body;\n\n    return RTMP_SendPacket(r, &packet, FALSE);\n}\n```\n\n要求メッセージ:\n\n|フィールド名|データ型|入力内容|\n|-|-|-|\n|コマンド名|String|releaseStream|\n|トランザクション ID|Number|2.<br>Invoke(connect) に割り振られた値より 1 多い値を割り振るようだ.|\n|Null|Null|AMF における Null.<br>コマンドオブジェクトなどを入力しない場合はトランザクション ID の直後にこの値を 1 つ入力するようだ.|\n|**playpath**|String|mp4やmp3などのファイル名. mp4: などのプリフィックスを付けることができる.<br>起動時に渡される URL から参照する.<br>そのパターンは次の通りである: protocol://server[:port][/app][/playpath]|\n\n応答メッセージ:\n\n|フィールド名|データ型|入力内容|\n|-|-|-|\n|コマンド名|String|\\_result<br>もしくは<br>\\_error|\n|トランザクション ID|Number|2.<br>Invoke(releaseStream) チャンクに入力されているものと同じものを使用する.|\n|Null|Null|AMF における Null.<br>プロパティなどを入力しない場合はこの値を 1 つ入力するようだ.|\n\n2. Invoke(FCPublish)\n\nInvoke(FCPublish) チャンクとその応答メッセージは FFmpeg および OBS によると以下の構造であるようだ.\n\nFFmpeg/rtmpproto.c#L641-L663[^FFmpeg/rtmpproto.c#L641-L663]\nFFmpeg/rtmpproto.c#L1956-L1965[^FFmpeg/rtmpproto.c#L1956-L1965]\n\n```c\n/**\n * Generate \'FCPublish\' call and send it to the server. It should make\n * the server prepare for receiving media streams.\n */\nstatic int gen_fcpublish_stream(URLContext *s, RTMPContext *rt)\n{\n    RTMPPacket pkt;\n    uint8_t *p;\n    int ret;\n\n    if ((ret = ff_rtmp_packet_create(&pkt, RTMP_SYSTEM_CHANNEL, RTMP_PT_INVOKE,\n                                     0, 25 + strlen(rt->playpath))) < 0)\n        return ret;\n\n    av_log(s, AV_LOG_DEBUG, "FCPublish stream...\\n");\n    p = pkt.data;\n    ff_amf_write_string(&p, "FCPublish");\n    ff_amf_write_number(&p, ++rt->nb_invokes);\n    ff_amf_write_null(&p);\n    ff_amf_write_string(&p, rt->playpath);\n\n    return rtmp_send_packet(rt, &pkt, 1);\n}\n\n// 中略\n\nif (!strcmp(command, "FCPublish")) {\n    if ((ret = ff_rtmp_packet_create(&spkt, RTMP_SYSTEM_CHANNEL,\n                                     RTMP_PT_INVOKE, 0,\n                                     RTMP_PKTDATA_DEFAULT_SIZE)) < 0) {\n        av_log(s, AV_LOG_ERROR, "Unable to create response packet\\n");\n        return ret;\n    }\n    pp = spkt.data;\n    ff_amf_write_string(&pp, "onFCPublish");\n}\n```\n\nobs-studio/rtmp.c#L2020-L2046[^obs-studio/rtmp.c#L2020-L2046]\n\n```c\nstatic int\nSendFCPublish(RTMP *r, int streamIdx)\n{\n    RTMPPacket packet;\n    char pbuf[1024], *pend = pbuf + sizeof(pbuf);\n    char *enc;\n\n    packet.m_nChannel = 0x03;\t/* control channel (invoke) */\n    packet.m_headerType = RTMP_PACKET_SIZE_MEDIUM;\n    packet.m_packetType = RTMP_PACKET_TYPE_INVOKE;\n    packet.m_nTimeStamp = 0;\n    packet.m_nInfoField2 = 0;\n    packet.m_hasAbsTimestamp = 0;\n    packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE;\n\n    enc = packet.m_body;\n    enc = AMF_EncodeString(enc, pend, &av_FCPublish);\n    enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes);\n    *enc++ = AMF_NULL;\n    enc = AMF_EncodeString(enc, pend, &r->Link.streams[streamIdx].playpath);\n    if (!enc)\n        return FALSE;\n\n    packet.m_nBodySize = enc - packet.m_body;\n\n    return RTMP_SendPacket(r, &packet, FALSE);\n}\n```\n\n要求メッセージ:\n\n|フィールド名|データ型|入力内容|\n|-|-|-|\n|コマンド名|String|FCPublish|\n|トランザクション ID|Number|3.<br>Invoke(releaseStream) に割り振られた値より 1 多い値を割り振るようだ.|\n|Null|Null|AMF における Null.<br>コマンドオブジェクトなどを入力しない場合はトランザクション ID の直後にこの値を 1 つ入力するようだ.|\n|playpath|String|Invoke(releaseStream) に入力されたものと同じ値を入力する.|\n\n応答メッセージ:\n\n|フィールド名|データ型|入力内容|\n|-|-|-|\n|コマンド名|String|onFCPublish|\n\n3. Invoke(createStream)\n\nInvoke(createStream) チャンクは公式ドキュメント[^RTMP-Specification-1.0]では以下のように定義されている.\n\n要求メッセージ:\n\n|フィールド名|データ型|入力内容|\n|-|-|-|\n|コマンド名|String|createStream|\n|トランザクション ID|Number|コマンドのトランザクション ID.|\n|コマンドオブジェクト|Object<br>または<br>Null|当該コマンドに設定する情報がある場合は Invoke(connect) と同じフォーマットのコマンドオブジェクトを入力する.<br>そうでなければ AMF における Null を入力する.|\n\n応答メッセージ:\n\n|フィールド名|データ型|入力内容|\n|-|-|-|\n|コマンド名|String|\\_result<br>または<br>\\_error|\n|トランザクション ID.|Number|応答メッセージが属するコマンドの ID.|\n|コマンドオブジェクト|Object<br>または<br>Null|当該応答メッセージに設定する情報がある場合は Invoke(createStream) と同じフォーマットのコマンドオブジェクトを入力する.<br>そうでなければ AMF における Null を入力する.|\n|**メッセージストリーム ID**|Number|メッセージストリーム ID か**エラー情報が入力されたインフォメーションオブジェクト**を入力する.|\n\n一方で, FFmpeg および OBS では以下の構造であるようだ.\n\nFFmpeg/rtmpproto.c#L665-L687[^FFmpeg/rtmpproto.c#L665-L687]\nFFmpeg/rtmpproto.c#L1981-L1999[^FFmpeg/rtmpproto.c#L1981-L1999]\n\n```c\n/**\n * Generate \'createStream\' call and send it to the server. It should make\n * the server allocate some channel for media streams.\n */\nstatic int gen_create_stream(URLContext *s, RTMPContext *rt)\n{\n    RTMPPacket pkt;\n    uint8_t *p;\n    int ret;\n\n    av_log(s, AV_LOG_DEBUG, "Creating stream...\\n");\n\n    if ((ret = ff_rtmp_packet_create(&pkt, RTMP_SYSTEM_CHANNEL, RTMP_PT_INVOKE,\n                                     0, 25)) < 0)\n        return ret;\n\n    p = pkt.data;\n    ff_amf_write_string(&p, "createStream");\n    ff_amf_write_number(&p, ++rt->nb_invokes);\n    ff_amf_write_null(&p);\n\n    return rtmp_send_packet(rt, &pkt, 1);\n}\n\n// 中略\n\nif ((ret = ff_rtmp_packet_create(&spkt, RTMP_SYSTEM_CHANNEL,\n                                 RTMP_PT_INVOKE, 0,\n                                 RTMP_PKTDATA_DEFAULT_SIZE)) < 0) {\n    av_log(s, AV_LOG_ERROR, "Unable to create response packet\\n");\n    return ret;\n}\npp = spkt.data;\nff_amf_write_string(&pp, "_result");\nff_amf_write_number(&pp, seqnum);\nff_amf_write_null(&pp);\nif (!strcmp(command, "createStream")) {\n    rt->nb_streamid++;\n    if (rt->nb_streamid == 0 || rt->nb_streamid == 2)\n        rt->nb_streamid++; /* Values 0 and 2 are reserved */\n    ff_amf_write_number(&pp, rt->nb_streamid);\n    /* By now we don\'t control which streams are removed in\n     * deleteStream. There is no stream creation control\n     * if a client creates more than 2^32 - 2 streams. */\n}\n```\n\nobs-studio/rtmp.c#L1899-L1922[^obs-studio/rtmp.c#L1899-L1922]\n\n```c\nint\nRTMP_SendCreateStream(RTMP *r)\n{\n    RTMPPacket packet;\n    char pbuf[256], *pend = pbuf + sizeof(pbuf);\n    char *enc;\n\n    packet.m_nChannel = 0x03;\t/* control channel (invoke) */\n    packet.m_headerType = RTMP_PACKET_SIZE_MEDIUM;\n    packet.m_packetType = RTMP_PACKET_TYPE_INVOKE;\n    packet.m_nTimeStamp = 0;\n    packet.m_nInfoField2 = 0;\n    packet.m_hasAbsTimestamp = 0;\n    packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE;\n\n    enc = packet.m_body;\n    enc = AMF_EncodeString(enc, pend, &av_createStream);\n    enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes);\n    *enc++ = AMF_NULL;\t\t/* NULL */\n\n    packet.m_nBodySize = enc - packet.m_body;\n\n    return RTMP_SendPacket(r, &packet, TRUE);\n}\n```\n\n要求メッセージ:\n\n|フィールド名|データ型|入力内容|\n|-|-|-|\n|コマンド名|String|createStream|\n|トランザクション ID|Number|4.<br>Invoke(FCPublish) に割り振られた値より 1 多い値を割り振るようだ.|\n|Null|Null|AMF における Null.|\n\n応答メッセージ:\n\n|フィールド名|データ型|入力内容|\n|-|-|-|\n|コマンド名|String|\\_result<br>または<br>\\_error|\n|トランザクション ID|Number|4.<br>Invoke(createStream) チャンクに入力されている値と同じ値を入力するようだ.|\n|Null|Null|AMF における Null.|\n|**メッセージストリーム ID**|Number|サーバ側がクライアント側に割り振るメッセージストリーム ID.|\n\nInvoke(releaseStream), Invoke(FCPublish) および Invoke(createStream) の 3 つのチャンクへの応答をすべて終えると, クライアント側はサーバ側に Invoke(publish) チャンクを送信する.\n\n#### Invoke(publish)\n\nInvoke(publish) チャンクは公式ドキュメント[^RTMP-Specification-1.0]では以下のように定義されている.\n\n要求メッセージ:\n\n|フィールド名|データ型|入力内容|\n|-|-|-|\n|コマンド名|String|publsh|\n|トランザクション ID|Number|0|\n|コマンドオブジェクト|Null|publish コマンドにコマンドオブジェクトは存在しないので AMF における Null を入力する.|\n|発行名|String|ストリームの発行に使用される名前.|\n|発行の種類|String|live, record, append のいずれか.  \\\n||| * record: ストリームが発行され, データが新しいファイルに記録される. ファイルはサーバーアプリケーションを含むディレクトリ内のサブディレクトリのサーバーに保存される. ファイルが既に存在する場合, 上書きされる.  \\\n||| * append: ストリームが発行され、データがファイルに追加される. ファイルが見つからなかった場合, 作成される.  \\\n||| * live: ライブデータはファイルに記録せずに発行される.|\n\n応答メッセージ:\n\n|フィールド名|データ型|入力内容|\n|-|-|-|\n|コマンド名|String|onStatus|\n|トランザクション ID|Number|0|\n|コマンドオブジェクト|Null|onStatusメッセージにコマンドオブジェクトは存在しないので AMF における Null を入力する.|\n|インフォメーションオブジェクト|Object|少なくとも以下の 3 つのプロパティを持つオブジェクト:  \\\n||| * level: warning, status, error のいずれか.  \\\n||| * code: メッセージのステータスコード. 例えば NetStream.Play.Start.  \\\n||| * description: メッセージの人間が読める記述.  \\\n|||  \\\n|||インフォメーションオブジェクトは code に応じて他のプロパティを含め**てもよい**.|\n\n一方で, FFmpeg および OBS では以下の構造であるようだ.\n\nFFmpeg/rtmpproto.c#L838-L863[^FFmpeg/rtmpproto.c#L838-L863]\nFFmpeg/rtmpproto.c#L1858-L1899[^FFmpeg/rtmpproto.c#L1858-L1899]\n\n```c\n/**\n * Generate \'publish\' call and send it to the server.\n */\nstatic int gen_publish(URLContext *s, RTMPContext *rt)\n{\n    RTMPPacket pkt;\n    uint8_t *p;\n    int ret;\n\n    av_log(s, AV_LOG_DEBUG, "Sending publish command for \'%s\'\\n", rt->playpath);\n\n    if ((ret = ff_rtmp_packet_create(&pkt, RTMP_SOURCE_CHANNEL, RTMP_PT_INVOKE,\n                                     0, 30 + strlen(rt->playpath))) < 0)\n        return ret;\n\n    pkt.extra = rt->stream_id;\n\n    p = pkt.data;\n    ff_amf_write_string(&p, "publish");\n    ff_amf_write_number(&p, ++rt->nb_invokes);\n    ff_amf_write_null(&p);\n    ff_amf_write_string(&p, rt->playpath);\n    ff_amf_write_string(&p, "live");\n\n    return rtmp_send_packet(rt, &pkt, 1);\n}\n\nstatic int write_status(URLContext *s, RTMPPacket *pkt,\n                        const char *status, const char *filename)\n{\n    RTMPContext *rt = s->priv_data;\n    RTMPPacket spkt = { 0 };\n    char statusmsg[128];\n    uint8_t *pp;\n    int ret;\n\n    if ((ret = ff_rtmp_packet_create(&spkt, RTMP_SYSTEM_CHANNEL,\n                                     RTMP_PT_INVOKE, 0,\n                                     RTMP_PKTDATA_DEFAULT_SIZE)) < 0) {\n        av_log(s, AV_LOG_ERROR, "Unable to create response packet\\n");\n        return ret;\n    }\n\n    pp = spkt.data;\n    spkt.extra = pkt->extra;\n    ff_amf_write_string(&pp, "onStatus");\n    ff_amf_write_number(&pp, 0);\n    ff_amf_write_null(&pp);\n\n    ff_amf_write_object_start(&pp);\n    ff_amf_write_field_name(&pp, "level");\n    ff_amf_write_string(&pp, "status");\n    ff_amf_write_field_name(&pp, "code");\n    ff_amf_write_string(&pp, status);\n    ff_amf_write_field_name(&pp, "description");\n    snprintf(statusmsg, sizeof(statusmsg),\n             "%s is now published", filename);\n    ff_amf_write_string(&pp, statusmsg);\n    ff_amf_write_field_name(&pp, "details");\n    ff_amf_write_string(&pp, filename);\n    ff_amf_write_object_end(&pp);\n\n    spkt.size = pp - spkt.data;\n    ret = ff_rtmp_packet_write(rt->stream, &spkt, rt->out_chunk_size,\n                               &rt->prev_pkt[1], &rt->nb_prev_pkt[1]);\n    ff_rtmp_packet_destroy(&spkt);\n\n    return ret;\n}\n```\n\nobs-studio/rtmp.c#L2081-L2112[^obs-studio/rtmp.c#L2081-L2112]\n\n```c\nstatic int\nSendPublish(RTMP *r, int streamIdx)\n{\n    RTMPPacket packet;\n    char pbuf[1024], *pend = pbuf + sizeof(pbuf);\n    char *enc;\n\n    packet.m_nChannel = 0x04;\t/* source channel (invoke) */\n    packet.m_headerType = RTMP_PACKET_SIZE_LARGE;\n    packet.m_packetType = RTMP_PACKET_TYPE_INVOKE;\n    packet.m_nTimeStamp = 0;\n    packet.m_nInfoField2 = r->Link.streams[streamIdx].id;\n    packet.m_hasAbsTimestamp = 0;\n    packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE;\n\n    enc = packet.m_body;\n    enc = AMF_EncodeString(enc, pend, &av_publish);\n    enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes);\n    *enc++ = AMF_NULL;\n    enc = AMF_EncodeString(enc, pend, &r->Link.streams[streamIdx].playpath);\n    if (!enc)\n        return FALSE;\n\n    /* FIXME: should we choose live based on Link.lFlags & RTMP_LF_LIVE? */\n    enc = AMF_EncodeString(enc, pend, &av_live);\n    if (!enc)\n        return FALSE;\n\n    packet.m_nBodySize = enc - packet.m_body;\n\n    return RTMP_SendPacket(r, &packet, TRUE);\n}\n```\n\n要求メッセージ:\n\n|フィールド名|データ型|入力内容|\n|-|-|-|\n|コマンド名|String|publish|\n|トランザクション ID|Number|5.<br>Invoke(createStream) に割り振られた値より 1 多い値を割り振るようだ.|\n|コマンドオブジェクト|Null|AMF における Null.|\n|発行名|String|playpath と同じ値.|\n|発行の種類|String|live|\n\n応答メッセージ:\n\n|フィールド名|データ型|入力内容|\n|-|-|-|\n|コマンド名|String|onStatus|\n|トランザクション ID|Number|0|\n|コマンドオブジェクト|Null|AMF における Null.|\n|インフォメーションオブジェクト|Object|以下の名前と値のペア.  \\\n||| * level: status  \\\n||| * code: 何らかのステータスコード. FFmpeg/rtmpproto.c#L1965-L1973[^FFmpeg/rtmpproto.c#L1965-L1973] より, 今回は NetStream.Publish.Start が入力されている.  \\\n||| * description: "**playpath** is now published".  \\\n||| * details: playpath と同じ値.|\n\nInvoke(publish) チャンクの現在の仕様は, 要求メッセージのトランザクション ID が 0 でないことを除き RTMP 1.0 当時と同じようだ.\n\n上記の仕様に従い, クライアント/サーバ側は当該要求/応答チャンクを送信する. その手順は公式ドキュメント[^RTMP-Specification-1.0]では以下のように定義されている.\n\n<div id="rtmp-invoke-publish-sequences-official">\n\n@startuml\n== Invoke(createStream) への応答が完了した. ==\nクライアント -> サーバ: Invoke(publish)\nサーバ -> クライアント: User Control(Stream Begin)\nクライアント -> サーバ: Metadata\nクライアント -> サーバ: Audio\nクライアント -> サーバ: Chunk Size\nサーバ -> クライアント: Invoke(onStatus)\nクライアント -> サーバ ++ : Video\n== ストリームの送信が完了するまで. ==\n@enduml\n\n</div>\n\n図7. 公式が説明している Invoke(publish) のシーケンス {#caption-rtmp-invoke-publish-sequences-official}\n\n1. クライアント側はサーバ側に Invoke(publish) チャンクを送信する.\n2. サーバ側はクライアント側に User Control(Stream Begin) チャンクを送信する.\n3. クライアント側はサーバ側に Metadata チャンク, Audio/Video チャンクおよび Chunk Size チャンクを送信する.\n4. サーバ側はクライアント側に Invoke(onStatus) チャンクを送信する.\n5. クライアント側はストリームの送信が完了するまでサーバ側に映像/音声データを送信する.\n\n一方で, FFmpeg では以下の実装を行っている.\n\nFFmpeg/rtmpproto.c#L1965-L1973[^FFmpeg/rtmpproto.c#L1965-L1973]\n\n```c\nif (!strcmp(command, "publish")) {\n    ret = write_begin(s);\n    if (ret < 0)\n        return ret;\n\n    // Send onStatus(NetStream.Publish.Start)\n    return write_status(s, pkt, "NetStream.Publish.Start",\n                       filename);\n}\n```\n\n<div id="rtmp-invoke-publish-sequences-ffmpeg">\n\n@startuml\n== Invoke(createStream) への応答が完了した. ==\nクライアント -> サーバ: Invoke(publish)\nサーバ -> クライアント: User Control(Stream Begin)\nサーバ -> クライアント: Invoke(onStatus)\n== 映像/音声の送受信を開始する. ==\nクライアント -> サーバ ++ : Metadata\nクライアント -> サーバ: Audio\nクライアント -> サーバ: Video\n== 映像/音声の送信が完了するまで. ==\n@enduml\n\n</div>\n\n図8. FFmpeg が行っている Invoke(publish) のシーケンス {#caption-rtmp-invoke-publish-sequences-ffmpeg}\n\n1. クライアント側はサーバ側に Invoke(publish) チャンクを送信する.\n2. サーバ側はクライアント側に User Control(Stream Begin) チャンクと Invoke(onStatus) チャンクを送信する.\n3. クライアント側はストリームの送信が完了するまでサーバ側に映像/音声データを送信する.\n\nこちらも RTMP 1.0 当時に対して手順が変わっていることを確認できる. 上記の手順に従い Invoke(onStatus) チャンクの送信を終えると, クライアント側はサーバ側に Metadata チャンクを含めた Audio/Video チャンクの送信を開始する.\n\nここで, 上記の各種接続手順より現在の RTMP 層で映像/音声データを送受信するまでに必要な手順は以下に要約できる.\n\n### 現在の RTMP 接続の流れ\n\n<div id="rtmp-connection-sequences-current">\n\n@startuml\n== TCP 接続に成功した. ==\nクライアント -> サーバ: C0+C1\nサーバ -> クライアント: S0+S1+S2\nクライアント -> サーバ: C2\n== RTMP ハンドシェイクが完了した. ==\nクライアント -> サーバ: Invoke(connect)\nサーバ -> クライアント: Invoke(_result)\nサーバ -> クライアント: Window Acknowledgement Size / Server BandWidth\nサーバ -> クライアント: Set Peer BandWidth / Client BandWidth\nサーバ -> クライアント: User Control(Stream Begin)\nサーバ -> クライアント: Chunk Size\nサーバ -> クライアント: Invoke(_result)\n== アプリケーション接続が完了した. ==\nクライアント -> サーバ: Invoke(releaseStream)\nクライアント -> サーバ: Invoke(FCPublish)\nクライアント -> サーバ: Invoke(createStream)\nサーバ -> クライアント: Invoke(_result)\nサーバ -> クライアント: Invoke(onFCPublish)\nサーバ -> クライアント: Invoke(_result)\n== メッセージストリーム ID の付番が完了した. ==\nクライアント -> サーバ: Invoke(publish)\nサーバ -> クライアント: User Control(Stream Begin)\nサーバ -> クライアント: Invoke(onStatus)\n== 映像/音声の送受信を開始する. ==\nクライアント -> サーバ ++ : Metadata\nクライアント -> サーバ: Audio\nクライアント -> サーバ: Video\n== 映像/音声の送信が完了するまで. ==\n@enduml\n\n</div>\n\n図9. RTMP 全体の現在の*大まかな*シーケンス {#caption-rtmp-connection-sequences-current}\n\n1. クライアント側とサーバ側は TCP 層での接続の後, RTMP 層でのハンドシェイクを行う.\n   1. クライアント側はサーバ側に C0 チャンクと C1 チャンクを送信する.\n   2. サーバ側は, クライアント側から受信した C0 チャンクに入力されたプロトコルのバージョンに対応していれば S0 チャンクと S1 チャンク に C1 チャンクを添えて返送する.\n   3. クライアント側は, サーバ側から返送された S0 チャンクに入力されたプロトコルのバージョンに対応しており, C1 チャンクの内容が返送前と同じであれば, S1 チャンクを返送する.\n   4. サーバ側は, クライアント側から返送された S1 チャンクの内容が返送前と同じであればアプリケーション間接続を開始する.\n   5. Flash Player 9 / Adobe Media Server 3 以降の場合, クライアント/サーバ側は, 返送チャンクの受信時に C1/S1 チャンクに埋め込んだ [HMAC-SHA256 ダイジェスト](#fp9)と所定の鍵で求めたハッシュでも同一性を検証する必要がある.\n2. 1 で RTMP 層でのハンドシェイクが成功したなら, アプリケーション間接続に必要な情報を相互に伝達しあう.\n   1. クライアント側はサーバ側に Invoke(connect) チャンクを送信する.\n   2. サーバ側はクライアント側から受信した Invoke(connect) をデコードし, それが妥当であれば応答チャンクを送信する. 応答チャンクおよびそれに付随する各種チャンクの送信順序は以下の通りである.\n      1. Invoke(\\_result) チャンク\n      2. Window Acknowledgement Size / Server BandWidth チャンク\n      3. Set Peer BandWidth / Client BandWidth チャンク\n      4. User Control (Stream Begin) チャンク\n      5. Chunk Size チャンク\n      6. Invoke(\\_result) チャンク\n   3. クライアント側はサーバ側から応答チャンクを受信したなら, 以下の手順で Invoke(createStream) チャンクとそれに付随するチャンクを同時に送信する.\n      1. Invoke(releaseStream) チャンク\n      2. Invoke(FCPublish) チャンク\n      3. Invoke(createStream) チャンク\n   4. サーバ側はクライアント側から受信した上記のチャンクをデコードし, それらが妥当であれば各種応答チャンクを送信し, メッセージに一意な値を付番する.\n   5. クライアント側はサーバ側から Invoke(createStream) チャンクに対する応答チャンクを受信したなら, Invoke(publish) チャンクを送信し, サーバ側に映像/音声データの送受信開始を伝える.\n   6. サーバ側はクライアント側から受信した Invoke(publish) チャンクをデコードし, それが妥当であれば User Control(StreamBegin) チャンクと Invoke(onStatus) チャンクをクライアント側に送信する.\n3. 映像/音声データの送受信を行う.\n\n### パケットのメッセージフォーマット\n\nInvoke, Metadata および Shared Object の 3 種のチャンクデータには, AMF0 もしくは AMF3 のメッセージフォーマットが適用されている. それらの内訳は AMF のドキュメントでは以下のように定義されている.\n\n#### AMF0\n\n* AMF0[^AMF0-File-Format-Specification]\n\n|ID|データ型|サイズ|入力内容|\n|-|-|-|-|\n|0|Number|8 bytes|8 bytes 浮動小数点数のバイナリ表記.<br>受信時は 8 bytes のバイト列なので, それを浮動小数点数に変換する工夫が必要である.|\n|1|Boolean|1 byte|1 byte 整数.<br>0 を false, それ以外(一般的には 1)を true として扱う.|\n|2|String|2 bytes<br>+<br>可変(最大 (2^16^ - 1) bytes)|UTF-8 の文字列.<br>最初の 2 bytes には続く文字列の長さを入力する.|\n|3|Anonymous Object|可変|名前と値(どちらも AMF0 でエンコードされたもの)のペア.|\n|4|Movieclip| |サポートされておらず未来の使用のために予約されている.|\n|5|Null|0 bytes|なし.<br>ID のみを入力する.|\n|6|Undefined|0 bytes|なし.<br>ID のみを入力する.|\n|7|Reference|2 bytes|符号なし整数.<br>以下の 4 つのデータ型は複合することができる.  \\\n|||| * Anonymous Object  \\\n|||| * Typed Object  \\\n|||| * Strict Array \\\n|||| * ECMA Array  \\\n||||  \\\n||||複合オブジェクトの同じ実体が 1 つのオブジェクトグラフとして複数回現れるなら, それは参照として送られなければならない.<br>Reference 型は直列化されたオブジェクトのテーブル内でのインデックスを指す 2 bytes の符号なし整数である.<br>0 オリジンである.|\n|8|ECMA Array|4 bytes<br>+<br>可変(最大 (2^32^ - 1) 要素)|要素の総数(4 bytes)とその数と等しい数の名前と値(どちらも AMF0 でエンコードされたもの)のペア.<br>ECMA 配列もしくは連想配列である.<br>順序やすべてのインデックスはすべて文字列のキーとして扱われる.<br>シリアライズの観点で当該データ型は Anonymous Object 型と類似している.|\n|9|Object End|0 byte|なし.<br>ID のみを入力する.<br>以下の 4 つのデータ型はそれ自体の終わりの印として当該データ型の ID をその末尾に入力する.  \\\n|||| * Anonymous Object  \\\n|||| * Typed Object  \\\n|||| * ECMA Array  \\\n|||| * Strict Array  \\\n||||  \\\n||||ただし, 当該データ型は **ID の直前の 2 bytes に空白(0x0000)が存在しており, 3 bytes の ID として評価しないと Number 型と混同してしまう**ので注意が必要である.|\n|10|Strict Array|4 bytes<br>+<br>可変(最大 (2^32^ - 1) 要素)|要素の総数(4 bytes)とその数と等しい数の(AMF0 でエンコードされた)値.<br>当該データ型は順序インデックスを持つ厳密な配列として扱う.|\n|11|Date|2 bytes<br>+<br>8 bytes|8 bytes 浮動小数点数のバイナリ表記.<br>UTC 基準のタイムスタンプを浮動小数点数として入力する.<br>最初の 2 bytes はタイムゾーンであるが, 予約済でありサポートされていないため 0 で埋めるべきである.|\n|12|Long String|4 bytes<br>+<br>可変(最大 (2^32^ - 1) bytes)|UTF-8 の文字列.<br>最初の 4 bytes には続く文字列の長さを入力する.|\n|13|Unsupported|0 byte|なし.<br>ID のみを入力する.<br>直列化できないデータ型に対して, 当該データ型の ID をそのデータ型の場所で使うことができる.|\n|14|RecordSet| |サポートされておらず未来の使用のために予約されている.|\n|15|XML|4 bytes<br>+<br>可変(最大 (2^32^ - 1) bytes)|UTF-8 の文字列.<br>ActionScript 1.0, 2.0 での XMLDocument および ActionScript 3.0 での flash.xml.XMLDocument が XML ドキュメントの DOM 表現を提供する.<br>ただし, 直列化ではドキュメントの文字列表現が使用される.|\n|16|Typed Object|可変|オブジェクトの名前(AMF0 での String)と名前と値(どちらも AMF0 でエンコードされたもの)のペア.|\n|17|AVM+|可変|**AMF3** の値.<br>AMF0 のフォーマットの中で AMF3 のデータを扱う時に入力する.|\n\n#### AMF3\n\n* AMF3[^AMF-File-Format-Spec]\n\n|ID|データ型|サイズ|入力内容|\n|-|-|-|-|\n|0|Undefined|0 byte|なし.<br>ID のみを入力する.<br>AVM 以外のエンドポイントでは未定義の概念がなく, 当該データ型をAMF での Null として扱う場合があることに注意が必要である.|\n|1|Null|0 byte|なし.<br>ID のみを入力する.|\n|2|False|0 byte|なし.<br>ID のみを入力する.<br>当該データ型は AMF3 における真偽値の**偽**として扱う.|\n|3|True|0 byte|なし.<br>ID のみを入力する.<br>当該データ型は AMF3 における真偽値の**真**として扱う.|\n|4|Integer|29 **bits**|[29 bits](#integer-型%2C-長さ%2C-要素の総数) の整数.<br>28 bits の範囲を上回ったり下回ったりする場合は, AMF3 の Double 型を用いてシリアライズされる.|\n|5|Double|8 bytes|8 bytes の浮動小数点数.<br>エンコード/デコードの方法は AMF0 の Number 型と同じである.|\n|6|String|29 bits<br>+<br>可変(最大 (2^28^ - 1) bytes)|UTF-8 の文字列.<br>最初の [29 bits](#integer-型%2C-長さ%2C-要素の総数) には参照値または文字列の長さを入力する.|\n|7|XMLDocument|29 bits<br>+<br>可変(最大 (2^28^ - 1) bytes)|UTF-8 でエンコードされた XML の文字列表現.<br>ActionScript 3 では新しい XML 型があるが, 古い XMLDocument 型が flash.xml.XMLDocument として言語に残されている.|\n|8|Date|29 bits<br>+<br>8 bytes|8 bytes の浮動小数点数のバイナリ表記.<br>UTC 基準のタイムスタンプを浮動小数点数として入力する.<br>最初の [29 bits](#integer-型%2C-長さ%2C-要素の総数) は AMF3 の Integer の値や String の長さと同じフォーマットであるが, 値(最下位ビットのフラグが 1)として入力する場合は残りの 28 bits には何も入力しない.|\n|9|Array|29 bits<br>+<br>可変|当該データ型には以下の 3 つのフォーマットがある.  \\\n|||| * 参照であることを示す [29 bits](#integer-型%2C-長さ%2C-要素の総数) のフォーマット  \\\n|||| * 要素の総数([29 bits](#integer-型%2C-長さ%2C-要素の総数))と, 空要素(1 -&gt; 値なし)のみの場合も含む ECMA (連想)配列<br>(名前と値(どちらも AMF3 でエンコードされたもの)のペア)  \\\n|||| * 順序インデックスによる厳密な配列  \\\n||||  \\\n||||ECMA 配列フォーマットの場合は空要素を最後の要素として 1 つ置かなければならない.<br>また, 厳密な配列フォーマットは ECMA 配列フォーマットの末尾の空要素に続いて入力し, 要素の総数を ECMA 配列フォーマットの要素の総数に加える.|\n|10|Object|可変|当該データ型には以下の 6 つのフォーマットがある.  \\\n|||| * 当該データ型の参照であることを示す [29 bits](#integer-型%2C-長さ%2C-要素の総数) のフォーマット  \\\n|||| * トレイトのメンバのバイナリであることを示す([29 bits](#トレイトのバイナリ%2C-メンバの総数%2C-トレイトの参照)) + トレイトの特質とメンバのバイナリのペア  \\\n|||| * トレイトの参照であることを示す [29 bits](#トレイトのバイナリ%2C-メンバの総数%2C-トレイトの参照) のフォーマット  \\\n|||| * トレイトのメンバの総数([29 bits](#トレイトのバイナリ%2C-メンバの総数%2C-トレイトの参照)) + トレイトの特質と(AMF3 でエンコードされた)メンバ名のペア  \\\n|||| * 1 つ以上のトレイトのメンバの(AMF3 でエンコードされた)値  \\\n|||| * 1 つ以上の**動的な**メンバ<br>(名前と値(どちらも AMF3 でエンコードされたもの)のペア)  \\\n||||  \\\n||||ここで, トレイトの特質とは以下の 4 つの内 1 つを指す AMF3 の文字列である.  \\\n|||| * anonymous<br>匿名のオブジェクト. 空文字("")を入力する.  \\\n|||| * typed<br>名前付きのオブジェクト.  \\\n|||| * dynamic<br>名前付き かつ 動的なメンバを持つオブジェクト.  \\\n|||| * externalizable<br>**外部のプログラムが変換可能な**名前を持つオブジェクト.  \\\n||||  \\\n||||**Object 型としての**参照値もしくはトレイトのフォーマットの直後にメンバの実際の値が続き, 動的なメンバがあれば更にその直後にそれが続く形になる.|\n|11|XML|29 bits<br>+<br>可変(最大 (2^28^ - 1) bytes)|UTF-8 でエンコードされた XML の文字列表現.|\n|12|ByteArray|29 bits<br>+<br>可変(最大 (2^28^ - 1) bytes)|1 byte 符号なし整数の配列.|\n|13|Vector(Int)|29 bits<br>+<br>1 byte<br>+<br>可変(最大 (2^28^ - 1) 要素)|4 byte **符号付き** 整数の配列.<br>[29 bits](#integer-型%2C-長さ%2C-要素の総数) の直後の 1 byte は 1 ならその配列が固定長であることを, 0 なら可変長であることを意味する.|\n|14|Vector(UInt)|^^|4 byte **符号なし** 整数の配列.<br>〃|\n|15|Vector(Double)|^^|8 byte **浮動小数点数**(のバイナリ表記)の配列.<br>〃|\n|16|Vector(Object)|^^|**AMF3 のデータ型**の配列.<br>固定長かどうかのフラグの直後に AMF3 のデータ型の名前(AMF3 でエンコードされた文字列)を入力し, 以降にその名前で表現される AMF3 データの実体を入力する.<br>〃|\n|17|Dictionary|29 bits<br>+<br>1 byte<br>+<br>可変(最大 (2^28 - 1) 要素)|名前と値(どちらも AMF3 でエンコードされたもの)のペア.<br>Object 型との違いは**キーを任意の AMF3 データ型にできる**ことである.<br>[29 bits](#integer-型%2C-長さ%2C-要素の総数) の直後の 1 byte は 1 ならキーが弱参照であることを, 0 ならそうでないことを意味する.|\n\nなお, AMF3 における 29 bits のフィールドの内訳は以下の通りである.\n\n##### Integer 型, 長さ, 要素の総数\n\n* 最下位ビットが 0 の場合\n\nそのデータは参照であり, 残りの 28 bits は参照テーブルのインデックス(整数)を入力する.\n\n* 〃 1 の場合\n\nそのデータは実際の値であり, 残りの 28 bits には続く文字列の長さなどの整数を入力する.\n\n##### トレイトのバイナリ, メンバの総数, トレイトの参照\n\n* バイナリの場合\n\n最下位 **3 bits** に **0b111** を入力する. メンバの総数として常に 0 を入力することになるため, 残りの 26 bits には意味がない.\n\n* **トレイトの**参照の場合\n\n最下位 **2 bits** に **0b01** を入力する. 2 bit 目は **Object 型のトレイトが**参照で送られていることを意味する値(0)である. 残りの 27 bits にはトレイトの参照のインデックスを入力する.\n\n* メンバの総数の場合\n\n最下位 **4 bits** に **0bX011** を入力する. X は 1 なら動的なトレイトである(動的なメンバを持つ)ことを, 0 ならそうでないことを意味する. 残りの 25 bits にはトレイト名の直後に入力する**静的なメンバの**総数を入力する.\n\n## 参考文献\n\n[^RTMP-Specification-1.0]: Adobe Systems Inc., "RTMP Specification 1.0", http://wwwimages.adobe.com/content/dam/Adobe/en/devnet/rtmp/pdf/rtmp_specification_1.0.pdf\n\n[^FFmpeg/rtmpproto.c#L1200-L1236]: FFmpeg, "FFmpeg/rtmpproto.c#L1200-L1236", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmpproto.c#L1200-L1236\n\n[^obs-studio/rtmp.c#L4062]: obsproject, "obs-studio/rtmp.c#L4062", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/rtmp.c#L4062\n\n[^obs-studio/handshake.h#L831-L837]: obsproject, "obs-studio/handshake.h#L831-L837", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/handshake.h#L831-L837\n\n[^red5-server-common/RTMPHandshake.java#L67]: Red5, "red5-server-common/RTMPHandshake.java#L67", https://github.com/Red5/red5-server-common/blob/v1.2.2/src/main/java/org/red5/server/net/rtmp/RTMPHandshake.java#L67\n\n[^FFmpeg/rtmpproto.c#L1200-L1207]: FFmpeg, "FFmpeg/rtmpproto.c#L1200-L1207", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmpproto.c#L1200-L1207\n\n[^FFmpeg/rtmp.h#L32-L41]: FFmpeg, "FFmpeg/rtmp.h#L32-L41", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmp.h#L32-L41\n\n[^obs-studio/handshake.h#L842-L865]: obsproject, "obs-studio/handshake.h#L842-L865", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/handshake.h#L842-L865\n\n[^red5-server/InboundHandshake.java#L348-L352]: Red5, "red5-server/InboundHandshake.java#L348-L352", https://github.com/Red5/red5-server/blob/v1.2.2/src/main/java/org/red5/server/net/rtmp/InboundHandshake.java#L348-L352\n\n[^FFmpeg/rtmpproto.c#L1248-L1258]: FFmpeg, "FFmpeg/rtmpproto.c#L1248-L1258", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmpproto.c#L1248-L1258\n\n[^obs-studio/rtmp.c#L4089-L4112]: obsproject, "obs-studio/rtmp.c#L4089-L4112", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/rtmp.c#L4089-L4112\n\n[^obs-studio/handshake.h#L936-L945]: obsproject, "obs-studio/handshake.h#L936-L945", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/handshake.h#L936-L945\n\n[^obs-studio/handshake.h#L1078-L1083]: obsproject, "obs-studio/handshake.h#L1078-L1083", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/handshake.h#L1078-L1083\n\n[^obs-studio/handshake.h#L1170-L1174]: obsproject, "obs-studio/handshake.h#L1170-L1174", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/handshake.h#L1170-L10174\n\n[^FFmpeg/rtmpproto.c#L1452-L1472]: FFmpeg, "FFmpeg/rtmpproto.c#L1452-L1472", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmpproto.c#L1452-L1472\n\n[^obs-studio/rtmp.c#L4152-L4178]: obsproject, "obs-studio/rtmp.c#L4152-L4178", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/rtmp.c#L4152-L4178\n\n[^obs-studio/handshake.h#L1442-L1447]: obsproject, "obs-studio/handshake.h#L1442-L1447", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/handshake.h#L1442-L1447\n\n[^obs-studio/handshake.h#L1524-L1528]: obsproject, "obs-studio/handshake.h#L1524-L1528", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/handshake.h#L1524-L1528\n\n[^red5-server/InboundHandshake.java#L213-L224]: Red5, "red5-server/InboundHandshake.java#L213-L224", https://github.com/Red5/red5-server/blob/v1.2.2/src/main/java/org/red5/server/net/rtmp/InboundHandshake.java#L213-L224\n\n[^red5-server/InboundHandshake.java#L304-L306]: Red5, "red5-server/InboundHandshake.java#L304-L306", https://github.com/Red5/red5-server/blob/v1.2.2/src/main/java/org/red5/server/net/rtmp/InboundHandshake.java#L304-L306\n\n[^FFmpeg/rtmpproto.c#L2347-L2395]: FFmpeg, "FFmpeg/rtmpproto.c#L2347-L2395", https://github.com/FFmpeg/FFmpeg/blob/n4.2/libavformat/rtmpproto.c#L2347-L2395\n\n[^obs-studio/rtmp.c#L1490-L1523]: obsproject, "obs-studio/rtmp.c#L1490-L1523", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/rtmp.c#L1490-L1523\n\n[^obs-studio/rtmp.c#L4972-L5059]: obsproject, "obs-studio/rtmp.c#L4972-L5059", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/rtmp.c#L4972-L5059\n\n[^red5-server-common/Aggregate.java#L119-L209]: Red5, "red5-server-common/Aggregate.java#L119-L209", https://github.com/Red5/red5-server-common/blob/v1.2.2/src/main/java/org/red5/server/net/rtmp/event/Aggregate.java#L119-L209\n\n[^FFmpeg/rtmpproto.c#L542-L575]: FFmpeg, "FFmpeg/rtmpproto.c#L542-L575", https://github.com/FFmpeg/FFmpeg/blob/n4.2/libavformat/rtmpproto.c#L542-L575\n\n[^FFmpeg/rtmpproto.c#L485-L588]: FFmpeg, "FFmpeg/rtmpproto.c#L485-L588", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmpproto.c#L485-L588\n\n[^FFmpeg/rtmppkt.c#L238-L244]: FFmpeg, "FFmpeg/rtmppkt.c#L238-L244", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmppkt.c#L238-L244\n\n[^obs-studio/rtmp.c#L3857-L4049]: obsproject, "obs-studio/rtmp.c#L3857-L4049", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/rtmp.c#L3857-L4049\n\n[^FFmpeg/rtmpproto.c#L593-L615]: FFmpeg, "FFmpeg/rtmpproto.c#L593-L615", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmpproto.c#L593-L615\n\n[^FFmpeg/rtmpproto.c#L1981-L1999]: FFmpeg, "FFmpeg/rtmpproto.c#L1981-L1999", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmpproto.c#L1981-L1999\n\n[^obs-studio/rtmp.c#L1990-L2016]: obsproject, "obs-studio/rtmp.c#L1990-L2016", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/rtmp.c#L1990-L2016\n\n[^FFmpeg/rtmpproto.c#L641-L663]: FFmpeg, "FFmpeg/rtmpproto.c#L641-L663", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmpproto.c#L641-L663\n\n[^FFmpeg/rtmpproto.c#L1956-L1965]: FFmpeg, "FFmpeg/rtmpproto.c#L1956-L1965", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmpproto.c#L1956-L1965\n\n[^obs-studio/rtmp.c#L2020-L2046]: obsproject, "obs-studio/rtmp.c#L2020-L2046", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/rtmp.c#L2020-L2046\n\n[^FFmpeg/rtmpproto.c#L665-L687]: FFmpeg, "FFmpeg/rtmpproto.c#L665-L687", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmpproto.c#L665-L687\n\n[^obs-studio/rtmp.c#L1899-L1922]: obsproject, "obs-studio/rtmp.c#L1899-L1922", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/rtmp.c#L1899-L1922\n\n[^FFmpeg/rtmpproto.c#L838-L863]: FFmpeg, "FFmpeg/rtmpproto.c#L838-L863", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmpproto.c#L838-L863\n\n[^FFmpeg/rtmpproto.c#L1858-L1899]: FFmpeg, "FFmpeg/rtmpproto.c#L1858-L1899", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmpproto.c#L1858-L1899\n\n[^obs-studio/rtmp.c#L2081-L2112]: obsproject, "obs-studio/rtmp.c#L2081-L2112", https://github.com/obsproject/obs-studio/blob/23.2.1/plugins/obs-outputs/librtmp/rtmp.c#L2081-L2112\n\n[^FFmpeg/rtmpproto.c#L1965-L1973]: FFmpeg, "FFmpeg/rtmpproto.c#L1965-L1973", https://github.com/FFmpeg/FFmpeg/blob/n4.2.1/libavformat/rtmpproto.c#L1965-L1973\n\n[^AMF0-File-Format-Specification]: Adobe Systems Inc., "AMF0 File Format Specification", https://www.adobe.com/content/dam/acom/en/devnet/pdf/amf0-file-format-specification.pdf\n\n[^AMF-File-Format-Spec]: Adobe Systems Inc., "AMF File Format Spec", https://www.adobe.com/content/dam/acom/en/devnet/pdf/amf-file-format-spec.pdf\n\n*[OSS]: Open Source Software\n*[RTMP]: Real-Time Messaging Protocol\n*[RTMPE]: Real-Time Messaging Protocol Encrypted\n*[RTMPS]: Real-Time Messaging Protocol over TLS/SSL\n*[RTMPT]: Real-Time Messaging Protocol over HTTP\n*[RTMPTE]: Real-Time Messaging Protocol Encrypted over HTTP\n*[RTMPTS]: Real-Time Messaging Protocol over HTTPS\n*[HTTP]: Hyper Text Transfer Protocol\n*[HTTPS]: HTTP over SSL\n*[TCP]: Transmission Control Protocol\n*[DH]: Diffie-Hellman\n*[TLS]: Transport Layer Security\n*[SSL]: Secure Socket Layer\n*[HLS]: HTTP Live Streaming\n*[OBS]: Open Broadcaster Software\n*[MPEG2-TS]: MPEG2 Transport Straming\n*[AVM]: ActionScript Virtual Machine\n\n',oe=t("d8e3"),pe=t.n(oe),ce="start=>start: TCP(1935) ポートを\n開放する.\nend=>end: 接続を閉じる.\nhandshake_tcp=>operation: TCP ハンドシェイク\nを行う.\nhandshake_rtmp=>operation: RTMP ハンドシェイク\nを行う.\napplication_connection=>operation: アプリケーション接続\nを行う.\nmessage_id_allocation=>operation: メッセージストリームに\nID を割り当てる.\npublishing=>operation: 映像/音声データを\n送受信する.\nhas_publishing_done=>condition: 映像/音声の送受信が\n完了した.\n\nstart->handshake_tcp->handshake_rtmp->application_connection->message_id_allocation->publishing->has_publishing_done\nhas_publishing_done(true)->end\nhas_publishing_done(false)->publishing\n",me={name:"Overview",components:{Report:en,Author:pn,ReportTitle:bn,Markdown:se},data:function(){return{title:"RTMP の概要",author:"T.Matsudate",published:"2019-09-09",source:ie}},mounted:function(){var n=pe.a.parse(ce);n.drawSVG("rtmp-connection-flows")}},de=me,le=(t("85c9"),Object(_["a"])(de,q,$,!1,null,null,null)),ue=le.exports,be=function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("Report",[t("Author",{attrs:{date:n.published,author:n.author},scopedSlots:n._u([{key:"author",fn:function(){},proxy:!0}])}),t("ReportTitle",{attrs:{"report-title":n.title},scopedSlots:n._u([{key:"title",fn:function(){},proxy:!0}])}),t("Markdown",{attrs:{source:n.source}})],1)},_e=[],he="## 記事が見つかりませんでした.\n\n以下の原因が考えられます.\n\n* 記事の URL が **kebab-case** ではない.\n* その記事がまだ公開されていないか, 削除されている.\n\n対処法:\n\n* 記事の URL を **kebab-case** で入力し直してみてください.\n* もしくはより新しく記事が公開されるまでお待ち下さい.\n",fe={name:"NotFound",components:{Report:en,Author:pn,ReportTitle:bn,Markdown:se},data:function(){return{title:"Report Not Found",author:"",published:"",source:he}}},ge=fe,Se=Object(_["a"])(ge,be,_e,!1,null,null,null),ke=Se.exports,Te=function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("article",{attrs:{id:"index"}},[n._m(0),t("h1",[n._v("目次")]),t("ol",[t("li",[t("h2",[t("router-link",{attrs:{to:"/rtmp-reports/overview"}},[n._v("RTMP の概要")])],1),n._m(1),t("p",[n._v("記載内容：")]),n._m(2)])]),n._m(3)])},Le=[function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("p",{attrs:{id:"published"}},[n._v("T.Matsudate"),t("br"),n._v("投稿日: "),t("time",{attrs:{datetime:"2019-09-09"}},[n._v("2019-09-09")])])},function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("p",[n._v("投稿日: "),t("time",{attrs:{datetime:"2019-09-09"}},[n._v("2019-09-09")])])},function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("ul",[t("li",[n._v("RTMP とは何か.")]),t("li",[n._v("基本的な通信手順について.")]),t("li",[n._v("既存製品はどのように通信しているか.")]),t("li",[n._v("私達はそれらの製品とどのように通信していけばよいか.")])])},function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{attrs:{id:"share-buttons"}},[t("a",{staticClass:"twitter-share-button",attrs:{href:"https://twitter.com/share?ref_src=twsrc%5Etfw","data-text":"RTMP Implementation Reports","data-url":"https://t-matsudate.github.io/rtmp-reports/","data-show-count":"true"}},[n._v("Tweet")])])}],ve={name:"Index"},Fe=ve,Me=(t("65d3"),Object(_["a"])(Fe,Te,Le,!1,null,null,null)),ye=Me.exports;r["a"].use(a["a"]),r["a"].config.productionTip=!1;var Pe=[{path:"/rtmp-reports",component:ye},{path:"/rtmp-reports/overview",component:ue},{path:"/rtmp-reports/*",component:ke}],Re=new a["a"]({routes:Pe,mode:"history"});new r["a"]({router:Re,render:function(n){return n(Z)}}).$mount("#app")},"58c6":function(n,e,t){"use strict";var r=t("4fec"),a=t.n(r);a.a},"65d3":function(n,e,t){"use strict";var r=t("ea22"),a=t.n(r);a.a},"6e86":function(n,e,t){},"6eee":function(n,e,t){"use strict";var r=t("7736"),a=t.n(r);a.a},7736:function(n,e,t){},"833d":function(n,e,t){},"85c9":function(n,e,t){"use strict";var r=t("833d"),a=t.n(r);a.a},"9b80":function(n,e,t){"use strict";var r=t("cd4e"),a=t.n(r);a.a},a901:function(n,e,t){"use strict";var r=t("2368"),a=t.n(r);a.a},c015:function(n,e,t){},cd4e:function(n,e,t){},cd50:function(n,e,t){var r={"./bmp":"0dcc","./bmp.js":"0dcc","./gif":"c416","./gif.js":"c416","./jpg":"135b","./jpg.js":"135b","./png":"e9ef","./png.js":"e9ef","./psd":"520c","./psd.js":"520c","./svg":"b0bf","./svg.js":"b0bf","./tiff":"f270","./tiff.js":"f270","./webp":"cf1e","./webp.js":"cf1e"};function a(n){var e=s(n);return t(e)}function s(n){if(!t.o(r,n)){var e=new Error("Cannot find module '"+n+"'");throw e.code="MODULE_NOT_FOUND",e}return r[n]}a.keys=function(){return Object.keys(r)},a.resolve=s,n.exports=a,a.id="cd50"},d3e5:function(n,e,t){},ea22:function(n,e,t){},eaf1:function(n,e,t){"use strict";var r=t("6e86"),a=t.n(r);a.a},eb39:function(n,e,t){"use strict";var r=t("d3e5"),a=t.n(r);a.a}});
//# sourceMappingURL=app.74d2173d.js.map